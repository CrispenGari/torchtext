{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PufgHqGWNPxt"
      },
      "source": [
        "### Sequence to Sequence (Seq2Seq)\n",
        "\n",
        "In this notebook we are going to create a germany to english translation model using `Seq2Seq` models.\n",
        "\n",
        "> This notebook is nothing but a modified version of my previous notebooks on `seq2seq` machine translation model. The only difference is that in this notebook we are going to use the `torchtext` api instead of the `torchtext.lergacy`. The following notebooks are used as refences to this notebook. There you will find a deep understanding on how `Seq2Seq` models work.\n",
        "\n",
        "1. [01_Sequence_To_Sequence_Introduction.ipynb](https://github.com/CrispenGari/pytorch-python/blob/main/09_NLP/03_Sequence_To_Sequence/01_Sequence_To_Sequence_Introduction.ipynb)\n",
        "\n",
        "\n",
        "### Installation of Packages\n",
        "In the following code cell we are going to install the packages that we are going to use in this notebook which are `helperfns` and `torchdata`. The `helperfns` package allows us to get some machine learning helper function which we are also going to use in this notebook as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOHX7hrENIkU",
        "outputId": "b887881b-3ed2-41d1-957e-730965106713"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.4 MB 46.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 140 kB 66.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 71.6 MB/s \n",
            "\u001b[?25h  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install helperfns torchdata -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z_Ck4gdOv3K"
      },
      "source": [
        "### Imports\n",
        "\n",
        "In the following code cell we are going to import the packages that are going to be used in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rafqZG3wNIhY",
        "outputId": "c6c35e94-1eb9-4172-e690-6f771d39f20f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('0.13.1', '1.12.1+cu113')"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "from torch import nn\n",
        "from torch.nn  import functional as F\n",
        "from torchtext import data, datasets\n",
        "from collections import Counter\n",
        "from torchtext import vocab\n",
        "from helperfns import tables, visualization, utils\n",
        "from helperfns.torch.models import model_params\n",
        "\n",
        "import spacy\n",
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torchtext\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "torchtext.__version__, torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okz1JiI6P5xV"
      },
      "source": [
        "### Seed\n",
        "In the following code cell we are going to set up the `SEED` for reproducivity in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8klW-wN0NId_"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deteministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu0y1XQ4QIGS"
      },
      "source": [
        "### Device\n",
        "\n",
        "In the following code cell we are going to declare a `device` variable that will allow us to make use of `GPU` if available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyGo-myfNIb-",
        "outputId": "73e913f0-39bf-477c-9243-05b157625adf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWDaqUDIQf4E"
      },
      "source": [
        "### Tokenizer Models\n",
        "\n",
        "For the tokenizer we are going to make use of the `spacy` language models to tokenize sentences for each language. So first we will need to download the `en_core_web_sm` and the `de_core_news_sm`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGD13NUYNIYb",
        "outputId": "0525de38-f90e-4fe8-807c-92a71b151c74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "spacy.cli.download('de_core_news_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gXc95YjQNIWm"
      },
      "outputs": [],
      "source": [
        "spacy_de = spacy.load('de_core_news_sm')\n",
        "spacy_en = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMvQzFTYRfZz"
      },
      "source": [
        "In the following code cell we are going to create our tokenizers functions that takes in a sentence in english and tokenize it.\n",
        "\n",
        "> Researchers find it beneficial to reverse the order of the input which they believe \"introduces many short term dependencies in the data that make the optimization problem much easier\". We copy this by reversing the German sentence after it has been transformed into a list of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Da_roQ2ONITL"
      },
      "outputs": [],
      "source": [
        "def tokenize_de(sent:str)->list:\n",
        "  \"\"\"\n",
        "  Tokenize german sentence and reverse it\n",
        "  \"\"\"\n",
        "  return [tok.text for tok in spacy_de.tokenizer(sent)][::-1]\n",
        "\n",
        "def tokenize_en(sent:str)->list:\n",
        "  return [tok.text for tok in spacy_en.tokenizer(sent)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81jcHtDkSGTr"
      },
      "source": [
        "Testing our tokenizer functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaE7za2mNIRX",
        "outputId": "5b0ca7a3-e521-4ba1-c0f2-468187e14af4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['?', 'helfen', 'mir', 'du', 'Kannst']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenize_de(\"Kannst du mir helfen?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5RROtSYNIOd",
        "outputId": "f5b4c0ae-b8c9-4b36-a5c4-0714b3e0d929"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Can', 'you', 'help', 'me', '?']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenize_en(\"Can you help me?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYv-xr3cSe7I"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The dataset we'll be using is the [Multi30k](https://pytorch.org/text/stable/datasets.html#multi30k) dataset. This is a dataset with `~30,000` parallel English, German and French sentences, each with `~12` words per sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "K6DayGR2NIMD"
      },
      "outputs": [],
      "source": [
        "train_iter, valid_iter, test_iter = datasets.Multi30k(\n",
        "     root = '.data', split = ('train', 'valid', 'test'), \n",
        "    language_pair= ('de', 'en')\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAmFsNzsSQoW"
      },
      "source": [
        "Checking as single train example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKuNV_MaNIIo",
        "outputId": "14694185-eee6-4f92-bcd7-3047bd31b62f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.',\n",
              " 'Two young, White males are outside near many bushes.')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(iter(train_iter))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCe9GrbgTZdC"
      },
      "source": [
        "### Getting `src` and `trg`\n",
        "Our `src` field will be the `de` sentence and the `trg` field will the the `en` sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-fOFrxOTNIGi"
      },
      "outputs": [],
      "source": [
        "train_src = []\n",
        "train_trg = []\n",
        "valid_src = []\n",
        "valid_trg = []\n",
        "test_src = []\n",
        "test_trg = []\n",
        "\n",
        "for (src, trg) in train_iter:\n",
        "  train_src.append(src)\n",
        "  train_trg.append(trg)\n",
        "\n",
        "for (src, trg) in test_iter:\n",
        "  test_src.append(src)\n",
        "  test_trg.append(trg)\n",
        "\n",
        "for (src, trg) in valid_iter:\n",
        "  valid_src.append(src)\n",
        "  valid_trg.append(trg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSIeQpwNSXGG"
      },
      "source": [
        "Checking if the `src` and the `trg` has the same length for all sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4LqMOxA2NIDd"
      },
      "outputs": [],
      "source": [
        "assert len(train_src) == len(train_trg), f\"The src and trg must have the same length got {len(train_src)} and {len(train_trg)}.\"\n",
        "assert len(valid_src) == len(valid_trg), f\"The src and trg must have the same length got {len(valid_src)} and {len(valid_trg)}.\"\n",
        "assert len(test_src) == len(test_trg), f\"The src and trg must have the same length got {len(test_src)} and {len(test_trg)}.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uKw4HcZUqK-"
      },
      "source": [
        "### Counting examples\n",
        "\n",
        "In the following code cell we are going to count examples in each language pair and visualize them using a table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSXbJRDHNIBW",
        "outputId": "efec0285-27a4-4d1e-c6f4-6d443718f577"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------+\n",
            "|          Examples          |\n",
            "+------------+-------+-------+\n",
            "|    set     |  src  |  trg  |\n",
            "+------------+-------+-------+\n",
            "| training   | 29001 | 29001 |\n",
            "| validation |  1015 |  1015 |\n",
            "| testing    |  1000 |  1000 |\n",
            "+------------+-------+-------+\n"
          ]
        }
      ],
      "source": [
        "columns = [\"set\", \"src\", \"trg\"]\n",
        "examples = [\n",
        "    ['training', len(train_src), len(train_trg)],\n",
        "    ['validation' , len(valid_src), len(valid_trg)],\n",
        "    ['testing' , len(test_src), len(test_trg)],\n",
        "]\n",
        "tables.tabulate_data(columns, examples, \"Examples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbnP0EM6VcIq"
      },
      "source": [
        "### Building Vocabulary\n",
        "Next, we'll build the vocabulary for the `source` (src) and `target` (trg) languages. The vocabulary is used to associate each unique token with an index (an integer). The vocabularies of the `source` and `target` languages are distinct.\n",
        "\n",
        "Using the `min_freq` argument, we only allow tokens that appear at least `2` times to appear in our vocabulary. Tokens that appear only once are converted into an `<unk>` (unknown) token.\n",
        "\n",
        "> It is important to note that our vocabulary should only be built from the `training` set and not the `validation/test` set. This prevents `\"information leakage\"` into our model, giving us artifically inflated `validation/test` scores.\n",
        "\n",
        "Also note that in this notebook we are not going to focus much about text cleaning. We are going to convert sentences to lower case only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "BOFgvANUNH-O"
      },
      "outputs": [],
      "source": [
        "counter_src = Counter()\n",
        "for line in train_src:\n",
        "  counter_src.update(tokenize_de(line.lower()))\n",
        "vocabulary_src = vocab.vocab(counter_src, min_freq=5, specials=('<unk>', '<sos>', '<eos>', '<pad>'))\n",
        "\n",
        "counter_trg = Counter()\n",
        "for line in train_trg:\n",
        "  counter_trg.update(tokenize_de(line.lower()))\n",
        "vocabulary_trg = vocab.vocab(counter_trg, min_freq=5, specials=('<unk>', '<sos>', '<eos>', '<pad>'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Swjhc1V8SnXj"
      },
      "source": [
        "In the following code cell we are going to get the `string-to-integer` representation of our `src` and `trg` fields."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Csd_5BBRNH8S"
      },
      "outputs": [],
      "source": [
        "stoi_src = vocabulary_src.get_stoi()\n",
        "stoi_trg = vocabulary_trg.get_stoi()\n",
        "\n",
        "SRC_VOCAB_SIZE = len(stoi_src)\n",
        "TRG_VOCAB_SIZE = len(stoi_trg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzRQmHw6Xe6M"
      },
      "source": [
        "### SRC and Target Pipelines\n",
        "After our text has been tokenized we need a way of converting those words into numbers because machine leaning models understand numbers not words. That's where we the `src_text_pipeline` and `trg_text_pipeline` functions into play. So these function takes in a sentence and tokenize it then converts each word to a number. Note that the word that does not exists in the vocabulay for either `src` or `trg` will be converted to an `unkown` (`<unk>`) token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6Zhq-2_9NH5B"
      },
      "outputs": [],
      "source": [
        "def src_text_pipeline(x: str)->list:\n",
        "  values = list()\n",
        "  tokens = tokenize_de(x.lower())\n",
        "  for token in tokens:\n",
        "    try:\n",
        "      v = stoi_src[token]\n",
        "    except KeyError as e:\n",
        "      v = stoi_trg['<unk>']\n",
        "    values.append(v)\n",
        "  return values\n",
        "\n",
        "def trg_text_pipeline(x: str)->list:\n",
        "  values = list()\n",
        "  tokens = tokenize_de(x.lower())\n",
        "  for token in tokens:\n",
        "    try:\n",
        "      v = stoi_trg[token]\n",
        "    except KeyError as e:\n",
        "      v = stoi_trg['<unk>']\n",
        "    values.append(v)\n",
        "  return values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vamzzJK5ZEOD"
      },
      "source": [
        "#### Translation Dataset\n",
        "\n",
        "In the following code cell we are going to create a `TranslationDataset` class which will inherid from the `torch.utils.data.Dataset` class. This class will take in the `src` and `trg` values and pair them together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_fEnlyUEZDi9"
      },
      "outputs": [],
      "source": [
        "class TranslationDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, src, trg):\n",
        "    super(TranslationDataset, self).__init__()\n",
        "    self.src = src\n",
        "    self.trg = trg\n",
        "      \n",
        "  def __getitem__(self, index):\n",
        "    return self.src[index], self.trg[index]\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.src)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5COsTvtUTaEl"
      },
      "source": [
        "### collate_fn of the `DataLoader`\n",
        "Our collate function will be named `tokenize_batch` which takes in the batch of `src` and `trg` pairs then `tokenize`, `numericalize` and then `pad` the to a given `max_len`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "rBepTZbdNH3Z"
      },
      "outputs": [],
      "source": [
        "def tokenize_batch(batch, max_len=100, padding=\"pre\"):\n",
        "  assert padding==\"pre\" or padding==\"post\", \"the padding can be either pre or post\"\n",
        "  src, trg = [], []\n",
        "  for src_, trg_ in batch:\n",
        "    _src = torch.zeros(max_len, dtype=torch.int32)\n",
        "    _trg = torch.zeros(max_len, dtype=torch.int32)\n",
        "    _src_processed_text = torch.tensor(src_text_pipeline(src_.lower()), dtype=torch.int32)\n",
        "    _trg_processed_text = torch.tensor(trg_text_pipeline(trg_.lower()), dtype=torch.int32)\n",
        "    _src_pos = min(max_len, len(_src_processed_text))\n",
        "    _trg_pos = min(max_len, len(_trg_processed_text))\n",
        "    if padding == \"pre\":\n",
        "      _src[:_src_pos] = _src_processed_text[:_src_pos]\n",
        "      _trg[:_trg_pos] = _trg_processed_text[:_trg_pos]\n",
        "    else:\n",
        "      _src[-_src_pos:] = _src_processed_text[-_src_pos:]\n",
        "      _trg[-_trg_pos:] = _trg_processed_text[-_trg_pos:]\n",
        "    trg.append(_trg.unsqueeze(dim=0))\n",
        "    src.append(_src.unsqueeze(dim=0))\n",
        "  #  the target values must be a LongTensor\n",
        "  return torch.cat(src, dim=0), torch.cat(trg, dim=0).type(torch.LongTensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrQYV7ZtUUCr"
      },
      "source": [
        "### Creating datasets\n",
        "In the following code cell we are going to create datasets for all our `3` sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "oInS_T66NH0Y"
      },
      "outputs": [],
      "source": [
        "train_dataset = TranslationDataset(train_src, train_trg)\n",
        "test_dataset = TranslationDataset(test_src, test_trg)\n",
        "valid_dataset = TranslationDataset(valid_src, valid_trg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0_NQd57UfeG"
      },
      "source": [
        "Checking a single example in the `train` set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmJVzPTRNHx3",
        "outputId": "a0171d62-5fb8-47b0-ce25-0012fdadc71e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.',\n",
              " 'Two young, White males are outside near many bushes.')"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUkDJkdAUmdG"
      },
      "source": [
        "### DataLoaders\n",
        "\n",
        "In the following code cell we are going to create  dataloaders for our three sets of data. We are going to pass the `tokenize_batch` as our `collate_fn`. We are going to shuffle example in the `train` set only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "PqHv_IxzNHuj"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=tokenize_batch)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=tokenize_batch)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=tokenize_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH9rAhXKVAjy"
      },
      "source": [
        "#### Checking examples in the train_loader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "nlvqqmBSNHsV"
      },
      "outputs": [],
      "source": [
        "src, trg = iter(train_loader).next()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILvF8HLbNHpT",
        "outputId": "821346fb-26d5-4ce4-da10-fc9c9072e14c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[   4, 1238,  602,  220,  226, 2463, 2946, 3228,   44,  365,   28,    0,\n",
              "          212,    6,    7,    8,   32,   36,   16,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0],\n",
              "        [   4, 2922, 2520,   30,  208,  120, 1509,   30,  226, 3458,  102, 2602,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0]], dtype=torch.int32)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "src[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIeDIhtZNHnL",
        "outputId": "28ba6b70-7407-4b2e-95a5-cad6197b78a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[   4,  465,   43,  246,  873, 2015,   35,  454,   43,   64,  294,   17,\n",
              "            7,  281,   37,   17,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0],\n",
              "        [   4, 3086,   67, 2702,  343,   42,  113,   67,  133,  849,   17, 1010,\n",
              "          601,   41,   21,   34, 3241,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0]])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trg[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO7m3irCV3HA"
      },
      "source": [
        "### Sequence To Sequence Model\n",
        "\n",
        "We are going to build our model in `3` parts. The `econder`, `decoder` and `seq2seq`. You can refer from [this notebook](https://github.com/CrispenGari/pytorch-python/blob/main/09_NLP/03_Sequence_To_Sequence/01_Sequence_To_Sequence_Introduction.ipynb).\n",
        "\n",
        "\n",
        "### Encoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "1mt6XJgaNEv4"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.hid_dim = hid_dim\n",
        "    self.n_layers = n_layers\n",
        "    self.embedding = nn.Embedding(input_dim, embedding_dim=emb_dim)\n",
        "    self.rnn = nn.LSTM(emb_dim, hid_dim ,n_layers, dropout=dropout)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, src):\n",
        "    # src = [src len, batch size]\n",
        "    embedded = self.dropout(self.embedding(src))\n",
        "    # embedded = [src len, batch size, emb dim]\n",
        "    outputs, (h_0, c_0) = self.rnn(embedded)\n",
        "\n",
        "    \"\"\"\n",
        "    outputs = [src len, batch size, hid dim * n directions]\n",
        "    hidden (h_0) = [n layers * n directions, batch size, hid dim]\n",
        "    cell (c_0) = [n layers * n directions, batch size, hid dim]\n",
        "    ** outputs are always from the top hidden layer\n",
        "    \"\"\"\n",
        "    return h_0, c_0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P4rU5fuWqZr"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "LaFQTP3VfyS6"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.output_dim = output_dim\n",
        "    self.hid_dim = hid_dim\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "    self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "    self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
        "    self.fc = nn.Linear(hid_dim, output_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, input, hidden, cell):\n",
        "    \"\"\"\n",
        "    input = [batch size]\n",
        "    hidden = [n layers * n directions, batch size, hid dim]\n",
        "    cell = [n layers * n directions, batch size, hid dim]\n",
        "    **n directions in the decoder will both always be 1, therefore:**\n",
        "      hidden = [n layers, batch size, hid dim]\n",
        "      context = [n layers, batch size, hid dim]\n",
        "    \"\"\"\n",
        "    input = input.unsqueeze(0)\n",
        "    # input = [1, batch size]\n",
        "\n",
        "    embedded = self.dropout(self.embedding(input))\n",
        "    # embedded = [1, batch size, emb dim]\n",
        "\n",
        "    output, (h_0, c_0) = self.rnn(embedded, (hidden, cell))\n",
        "    \"\"\"\n",
        "    output = [seq len, batch size, hid dim * n directions]\n",
        "    hidden = [n layers * n directions, batch size, hid dim]\n",
        "    cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "    **seq len and n directions will always be 1 in the decoder, therefore:**\n",
        "      output = [1, batch size, hid dim]\n",
        "      hidden (h_0) = [n layers, batch size, hid dim]\n",
        "      cell (c_0) = [n layers, batch size, hid dim]\n",
        "    \"\"\"\n",
        "    predictions = self.fc(output.squeeze(0))\n",
        "    #prediction = [batch size, output dim]\n",
        "    return predictions, h_0, c_0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI83ZV9gW7jp"
      },
      "source": [
        "###Seq2Seq (Sequence to Sequence)\n",
        "For the final part of the implemenetation, we'll implement the `seq2seq` model. This will handle:\n",
        "\n",
        "* receiving the input/source sentence\n",
        "* using the encoder to produce the context vectors\n",
        "* using the decoder to produce the predicted output/target sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "6s5GBR2cf5qq"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder, device):\n",
        "    super(Seq2Seq, self).__init__()\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.device = device\n",
        "\n",
        "    assert encoder.hid_dim == decoder.hid_dim, \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "    assert encoder.n_layers == decoder.n_layers, \"Encoder and decoder must have equal number of layers!\"\n",
        "\n",
        "  def forward(self, src, trg, teacher_forcing_ratio=.5):\n",
        "    \"\"\"\n",
        "    src = [src len, batch size]\n",
        "    trg = [trg len, batch size]\n",
        "    teacher_forcing_ratio is probability to use teacher forcing\n",
        "    e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "    \"\"\"\n",
        "    trg_len, batch_size = trg.shape\n",
        "    trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "    # tensor to store decoder outputs\n",
        "    outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "    # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "    hidden, cell = self.encoder(src)\n",
        "    # first input to the decoder is the <sos> tokens\n",
        "    input = trg[0,:]\n",
        "\n",
        "    for t in range(1, trg_len):\n",
        "      \"\"\"\n",
        "      insert input token embedding, previous hidden and previous cell states\n",
        "      receive output tensor (predictions) and new hidden and cell states\n",
        "      \"\"\"\n",
        "      output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "      # place predictions in a tensor holding predictions for each token\n",
        "      outputs[t] = output\n",
        "      # decide if we are going to use teacher forcing or not\n",
        "      teacher_force = random.random() < teacher_forcing_ratio\n",
        "      # get the highest predicted token from our predictions\n",
        "      top1 = output.argmax(1) \n",
        "      \"\"\"\n",
        "      if teacher forcing, use actual next token as next input\n",
        "      if not, use predicted token\n",
        "      \"\"\"\n",
        "      input = trg[t] if teacher_force else top1\n",
        "      return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lTf8TlkXaY4"
      },
      "source": [
        "### Training the `Seq2Seq`\n",
        "Now we have our model implemented, we can begin training it.\n",
        "\n",
        "First, we'll initialize our model. As mentioned before, the `input` and `output` dimensions are defined by the size of the vocabulary. The embedding dimesions and dropout for the encoder and decoder can be different, but the number of layers and the size of the hidden/cell states must be the same.\n",
        "\n",
        "We then define the encoder, decoder and then our `Seq2Seq` model, which we place on the device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baPnQEm-gF8Q",
        "outputId": "420d5b89-2e5f-46c1-b87c-efce3bc3474f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(3554, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(3275, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (fc): Linear(in_features=512, out_features=3275, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "INPUT_DIM = SRC_VOCAB_SIZE\n",
        "OUTPUT_DIM = TRG_VOCAB_SIZE\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECIe6GB4Xoyu"
      },
      "source": [
        "### Counting model parameters.\n",
        "We are going to use the `model_params` function from `helperfns` to count model parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdJM5MUagWd-",
        "outputId": "5664eeb3-7916-4c06-f880-4aa1803538f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TOTAL MODEL PARAMETERS: \t10784715\n",
            "TOTAL TRAINABLE PARAMETERS: \t10784715\n"
          ]
        }
      ],
      "source": [
        "model_params(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f_hhdNOYvHD"
      },
      "source": [
        "### Initializing weights to the model\n",
        "Next up is initializing the weights of our model.\n",
        "\n",
        "We initialize weights in pytorch by creating a function which we apply to our model. When using apply, the `init_weights` function will be called on every module and sub-module within our model. For each module we loop through all of the parameters and sample them from a uniform distribution with `nn.init.uniform_`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVbc6_fOge1Z",
        "outputId": "7ac82bd1-ab1e-40c3-c160-4689690d21a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(3554, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(3275, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (fc): Linear(in_features=512, out_features=3275, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def init_weights(m):\n",
        "  for name, param in m.named_parameters():\n",
        "    nn.init.uniform_(param.data, -0.8, 0.8)\n",
        "\n",
        "model.apply(init_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUPE1wJsZBos"
      },
      "source": [
        "### Optimizer\n",
        "For the optimizer we are going to use the `Adam` optimizer with default parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "qJDxezzQgeyv"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le7qmEbMZQEl"
      },
      "source": [
        "### Criterion\n",
        "Next, we define our loss function. The `CrossEntropyLoss` function calculates both the log softmax as well as the negative log-likelihood of our predictions.\n",
        "\n",
        "Our loss function calculates the average loss per token, however by passing the index of the `<pad>` token as the `ignore_index` argument we ignore the loss whenever the target token is a padding token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "ODrdgxuigev7"
      },
      "outputs": [],
      "source": [
        "TRG_PAD_IDX = stoi_trg[\"<pad>\"]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlUORlJ-Z36j"
      },
      "source": [
        "### Train Loop\n",
        "\n",
        "First, we'll set the model into \"training mode\" with `model.train()`. This will turn on dropout (and batch normalization, which we aren't using) and then iterate through our data iterator.\n",
        "\n",
        "At each iteration:\n",
        "\n",
        "* get the source and target sentences from the batch,  and put them to the `device` \n",
        "* zero the gradients calculated from the last batch\n",
        "* feed the source and target into the model to get the output, \n",
        "* as the loss function only works on 2d inputs with 1d targets we need to flatten each of them with `.view`\n",
        "  * we slice off the first column of the output and target tensors as mentioned above\n",
        "* calculate the gradients with `loss.backward()`\n",
        "* clip the gradients to prevent them from exploding (a common issue in RNNs)\n",
        "* update the parameters of our model by doing an optimizer step\n",
        "* sum the loss value to a running total\n",
        "* Finally, we return the loss that is averaged over all batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Nj8aeGdzgeuK"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "  model.train()\n",
        "  epoch_loss = 0\n",
        "  for i, (src, trg) in enumerate(iterator):\n",
        "    src = src.to(device)\n",
        "    trg = trg.type(torch.LongTensor).to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(src, trg)\n",
        "    \"\"\"\n",
        "    trg = [trg len, batch size]\n",
        "    output = [trg len, batch size, output dim]\n",
        "    \"\"\"\n",
        "    output_dim = output.shape[-1]\n",
        "    output = output[1:].view(-1, output_dim)\n",
        "    trg = trg[1:].view(-1)\n",
        "\n",
        "    \"\"\"\n",
        "    trg = [(trg len - 1) * batch size]\n",
        "    output = [(trg len - 1) * batch size, output dim]\n",
        "    \"\"\"\n",
        "    loss = criterion(output, trg)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv6TJ82padrH"
      },
      "source": [
        "### Evaluation Loop\n",
        "Our evaluation loop is similar to our training loop, however as we aren't updating any parameters we don't need to pass an `optimizer` or a `clip` value.\n",
        "\n",
        "We must remember to set the model to evaluation mode with `model.eval()`. This will turn off dropout (and batch normalization, if used).\n",
        "\n",
        "We use the with `torch.no_grad()` block to ensure no gradients are calculated within the block. This reduces memory consumption and speeds things up.\n",
        "\n",
        "The iteration loop is similar (without the parameter updates), however we must ensure we turn teacher forcing off for evaluation. This will cause the model to only use it's own predictions to make further predictions within a sentence, which mirrors how it would be used in deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "g_F2Wbwjgerf"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "  model.eval()\n",
        "  epoch_loss = 0\n",
        "  with torch.no_grad():\n",
        "    for i, (src, trg) in enumerate(iterator):\n",
        "      src = src.to(device)\n",
        "      trg = trg.type(torch.LongTensor).to(device)\n",
        "      output = model(src, trg, 0) #turn off teacher forcing\n",
        "      \"\"\"\n",
        "      trg = [trg len, batch size]\n",
        "      output = [trg len, batch size, output dim]\n",
        "      \"\"\"\n",
        "      output_dim = output.shape[-1]\n",
        "      output = output[1:].view(-1, output_dim)\n",
        "      trg = trg[1:].view(-1)\n",
        "      \"\"\"\n",
        "      trg = [(trg len - 1) * batch size]\n",
        "      output = [(trg len - 1) * batch size, output dim]\n",
        "      \"\"\"\n",
        "      loss = criterion(output, trg)\n",
        "      epoch_loss += loss.item()\n",
        "  return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ9AofNAgdhP"
      },
      "source": [
        "### Running the training loop.\n",
        "During training we are going to visualize our training metrics in tabular form. We are going to save the best model if and only if the previous validation loss is less greater than the current epoch validation loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIttAnWchY6f",
        "outputId": "8cb09947-9486-4d9d-b208-a4e797f71116"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------------------------+\n",
            "|     EPOCH: 01/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "|  CATEGORY  |  LOSS |   PPL    |    ETA     |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.036 |    1.037 | 0:00:02.57 |\n",
            "| Validation | 8.134 | 3410.024 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|         EPOCH: 02/10 not saving...         |\n",
            "+------------+-------+----------+------------+\n",
            "|  CATEGORY  |  LOSS |   PPL    |    ETA     |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.036 |    1.037 | 0:00:01.10 |\n",
            "| Validation | 8.134 | 3410.024 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|         EPOCH: 03/10 not saving...         |\n",
            "+------------+-------+----------+------------+\n",
            "|  CATEGORY  |  LOSS |   PPL    |    ETA     |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.036 |    1.037 | 0:00:01.06 |\n",
            "| Validation | 8.134 | 3410.024 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|         EPOCH: 04/10 not saving...         |\n",
            "+------------+-------+----------+------------+\n",
            "|  CATEGORY  |  LOSS |   PPL    |    ETA     |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.036 |    1.037 | 0:00:01.06 |\n",
            "| Validation | 8.134 | 3410.024 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|         EPOCH: 05/10 not saving...         |\n",
            "+------------+-------+----------+------------+\n",
            "|  CATEGORY  |  LOSS |   PPL    |    ETA     |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.036 |    1.037 | 0:00:01.05 |\n",
            "| Validation | 8.134 | 3410.024 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|         EPOCH: 06/10 not saving...         |\n",
            "+------------+-------+----------+------------+\n",
            "|  CATEGORY  |  LOSS |   PPL    |    ETA     |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.036 |    1.037 | 0:00:01.06 |\n",
            "| Validation | 8.134 | 3410.024 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|         EPOCH: 07/10 not saving...         |\n",
            "+------------+-------+----------+------------+\n",
            "|  CATEGORY  |  LOSS |   PPL    |    ETA     |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.036 |    1.037 | 0:00:01.07 |\n",
            "| Validation | 8.134 | 3410.024 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|         EPOCH: 08/10 not saving...         |\n",
            "+------------+-------+----------+------------+\n",
            "|  CATEGORY  |  LOSS |   PPL    |    ETA     |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.036 |    1.037 | 0:00:01.07 |\n",
            "| Validation | 8.134 | 3410.024 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|         EPOCH: 09/10 not saving...         |\n",
            "+------------+-------+----------+------------+\n",
            "|  CATEGORY  |  LOSS |   PPL    |    ETA     |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.036 |    1.037 | 0:00:01.08 |\n",
            "| Validation | 8.134 | 3410.024 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|         EPOCH: 10/10 not saving...         |\n",
            "+------------+-------+----------+------------+\n",
            "|  CATEGORY  |  LOSS |   PPL    |    ETA     |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.036 |    1.037 | 0:00:01.07 |\n",
            "| Validation | 8.134 | 3410.024 |            |\n",
            "+------------+-------+----------+------------+\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "best_valid_loss = float('inf')\n",
        "MODEL_NAME = 'best-model.pt'\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start = time.time()\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_loader, criterion)\n",
        "    title = f\"EPOCH: {epoch+1:02}/{N_EPOCHS:02} {'saving best model...' if valid_loss < best_valid_loss else 'not saving...'}\"\n",
        "    end = time.time()\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), MODEL_NAME)\n",
        "    data = [\n",
        "       [\"Training\", f'{train_loss:.3f}', f'{math.exp(train_loss):7.3f}', f\"{utils.hms_string(end - start)}\" ],\n",
        "       [\"Validation\", f'{valid_loss:.3f}', f'{math.exp(valid_loss):7.3f}', \"\" ],       \n",
        "   ]\n",
        "    columns = [\"CATEGORY\", \"LOSS\", \"PPL\", \"ETA\"]\n",
        "    tables.tabulate_data(columns, data, title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmGj2TFZcdO9"
      },
      "source": [
        "### Evaluating the Best Model\n",
        "\n",
        "In the following code cell we are going to evaluate the best model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncbUrUC3hZgG",
        "outputId": "3c725cfc-dcf9-4eba-fe0e-21688e98d040"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------------------+\n",
            "|       Model Evaluation Summary       |\n",
            "+------+-------+----------+------------+\n",
            "| Set  |  Loss |   PPL    | ETA (time) |\n",
            "+------+-------+----------+------------+\n",
            "| Test | 8.131 | 3399.412 |            |\n",
            "+------+-------+----------+------------+\n"
          ]
        }
      ],
      "source": [
        "column_names = [\"Set\", \"Loss\", \"PPL\", \"ETA (time)\"]\n",
        "model.load_state_dict(torch.load(MODEL_NAME))\n",
        "test_loss= evaluate(model, test_loader, criterion)\n",
        "title = \"Model Evaluation Summary\"\n",
        "data_rows = [[\"Test\", f'{test_loss:.3f}', f'{math.exp(test_loss):7.3f}', \"\"]]\n",
        "\n",
        "tables.tabulate_data(column_names, data_rows, title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzLCGynMc5eO"
      },
      "source": [
        "### What's Next?\n",
        "In the following notebook we'll implement a model that achieves improved test `perplexity`, but only uses a single layer in the encoder and the decoder."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
