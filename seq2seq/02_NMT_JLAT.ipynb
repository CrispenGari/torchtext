{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Neural Machine Translation by Jointly Learning to Align and Translate\n",
        "\n",
        "In this notebook we will implement the model from [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) that will improve PPL (perplexity) as compared to the previous notebook.\n",
        "\n",
        "> This notebook is nothing but a modified version of my previous notebooks on `seq2seq` machine translation model. The only difference is that in this notebook we are going to use the `torchtext` api instead of the `torchtext.lergacy`. The following notebooks are used as refences to this notebook. There you will find a deep understanding on how `Seq2Seq` models work.\n",
        "\n",
        "1. [03_Neural_Machine_Translation_by_Jointly_Learning_to_Align_and_Translate.ipynb](https://github.com/CrispenGari/pytorch-python/blob/main/09_NLP/03_Sequence_To_Sequence/03_Neural_Machine_Translation_by_Jointly_Learning_to_Align_and_Translate.ipynb)\n",
        "\n",
        "\n",
        "The model implemented in this notebook avoids this compression by allowing the decoder to look at the entire source sentence (via its hidden states) at each decoding step! How does it do this? It uses **attention**.\n",
        "\n",
        "### Attention.\n",
        "Attention works by first, calculating an attention vector, $a$ , that is the length of the source sentence. The attention vector has the property that each element is between `0` and `1`, and the entire vector sums to `1`. We then calculate a weighted sum of our source sentence hidden states, $H$ , to get a weighted source vector, $w$. You can read more about it in [this notebook]((https://github.com/CrispenGari/pytorch-python/blob/main/09_NLP/03_Sequence_To_Sequence/03_Neural_Machine_Translation_by_Jointly_Learning_to_Align_and_Translate.ipynb).\n",
        "\n",
        "\n",
        " \n",
        "We calculate a new weighted source vector every time-step when decoding, using it as input to our decoder RNN as well as the linear layer to make a prediction.\n",
        "\n",
        "### Installation of Packages\n",
        "In the following code cell we are going to install the packages that we are going to use in this notebook which are `helperfns` and `torchdata`. The `helperfns` package allows us to get some machine learning helper function which we are also going to use in this notebook as well."
      ],
      "metadata": {
        "id": "PufgHqGWNPxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "TaQjAQ78VIGq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install helperfns torchdata portalocker -q"
      ],
      "metadata": {
        "id": "jOHX7hrENIkU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a27458bf-f352-428c-c091-e840cd56df43"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports\n",
        "\n",
        "In the following code cell we are going to import the packages that are going to be used in this notebook."
      ],
      "metadata": {
        "id": "7Z_Ck4gdOv3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torch.nn  import functional as F\n",
        "from torchtext import data, datasets\n",
        "from collections import Counter\n",
        "from torchtext import vocab\n",
        "from helperfns import tables, visualization, utils\n",
        "from helperfns.torch.models import model_params\n",
        "\n",
        "import spacy\n",
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torchtext\n",
        "import numpy as np\n",
        "import time\n",
        "import gc\n",
        "\n",
        "torchtext.__version__, torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rafqZG3wNIhY",
        "outputId": "de569c8a-8c02-48bb-ef80-8c982d63ab78"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('0.15.2+cpu', '2.0.1+cu118')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seed\n",
        "In the following code cell we are going to set up the `SEED` for reproducivity in this notebook."
      ],
      "metadata": {
        "id": "okz1JiI6P5xV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deteministic = True"
      ],
      "metadata": {
        "id": "8klW-wN0NId_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Device\n",
        "\n",
        "In the following code cell we are going to declare a `device` variable that will allow us to make use of `GPU` if available."
      ],
      "metadata": {
        "id": "eu0y1XQ4QIGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyGo-myfNIb-",
        "outputId": "4a974eb9-0fe2-4aee-cdc3-c3648dbc2052"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer Models\n",
        "\n",
        "For the tokenizer we are going to make use of the `spacy` language models to tokenize sentences for each language. So first we will need to download the `en_core_web_sm` and the `de_core_news_sm`."
      ],
      "metadata": {
        "id": "gWDaqUDIQf4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.cli.download('de_core_news_sm')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGD13NUYNIYb",
        "outputId": "c8bcb77a-f89c-4860-fbe7-ac716c1d2bdb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_de = spacy.load('de_core_news_sm')\n",
        "spacy_en = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "gXc95YjQNIWm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following code cell we are going to create our tokenizers functions that takes in a sentence in english and tokenize it.\n",
        "\n",
        "> Previously we reversed the source (German) sentence, however in the paper we are implementing they don't do this, so neither will we."
      ],
      "metadata": {
        "id": "sMvQzFTYRfZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_de(sent:str)->list:\n",
        "  return [tok.text for tok in spacy_de.tokenizer(sent)]\n",
        "\n",
        "def tokenize_en(sent:str)->list:\n",
        "  return [tok.text for tok in spacy_en.tokenizer(sent)]"
      ],
      "metadata": {
        "id": "Da_roQ2ONITL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing our tokenizer functions."
      ],
      "metadata": {
        "id": "81jcHtDkSGTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_de(\"Kannst du mir helfen?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaE7za2mNIRX",
        "outputId": "6fee93c7-b98f-4f7d-9d6a-f6256a5bd628"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Kannst', 'du', 'mir', 'helfen', '?']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_en(\"Can you help me?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5RROtSYNIOd",
        "outputId": "07258d48-8625-4e4e-fc94-32d053de3a66"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Can', 'you', 'help', 'me', '?']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset\n",
        "\n",
        "The dataset we'll be using is the [Multi30k](https://pytorch.org/text/stable/datasets.html#multi30k) dataset. This is a dataset with `~30,000` parallel English, German and French sentences, each with `~12` words per sentence."
      ],
      "metadata": {
        "id": "HYv-xr3cSe7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter, valid_iter, test_iter = datasets.Multi30k(\n",
        "     root = '.data', split = ('train', 'valid', 'test'), \n",
        "    language_pair= ('de', 'en')\n",
        ")"
      ],
      "metadata": {
        "id": "K6DayGR2NIMD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking as single train example."
      ],
      "metadata": {
        "id": "AAmFsNzsSQoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_iter))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKuNV_MaNIIo",
        "outputId": "8d7abeb3-a524-4ea6-d0e0-a823477bd220"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Zwei junge weiÃŸe MÃ¤nner sind im Freien in der NÃ¤he vieler BÃ¼sche.',\n",
              " 'Two young, White males are outside near many bushes.')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting `src` and `trg`\n",
        "Our `src` field will be the `de` sentence and the `trg` field will the the `en` sentence."
      ],
      "metadata": {
        "id": "cCe9GrbgTZdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_src = []\n",
        "train_trg = []\n",
        "valid_src = []\n",
        "valid_trg = []\n",
        "test_src = []\n",
        "test_trg = []\n",
        "\n",
        "for (src, trg) in train_iter:\n",
        "  train_src.append(src)\n",
        "  train_trg.append(trg)\n",
        "\n",
        "for (src, trg) in test_iter:\n",
        "  test_src.append(src)\n",
        "  test_trg.append(trg)\n",
        "\n",
        "for (src, trg) in valid_iter:\n",
        "  valid_src.append(src)\n",
        "  valid_trg.append(trg)"
      ],
      "metadata": {
        "id": "-fOFrxOTNIGi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44aa870f-81dd-4a63-80e5-2d8e70e78b1c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/combining.py:297: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking if the `src` and the `trg` has the same length for all sets."
      ],
      "metadata": {
        "id": "wSIeQpwNSXGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert len(train_src) == len(train_trg), f\"The src and trg must have the same length got {len(train_src)} and {len(train_trg)}.\"\n",
        "assert len(valid_src) == len(valid_trg), f\"The src and trg must have the same length got {len(valid_src)} and {len(valid_trg)}.\"\n",
        "assert len(test_src) == len(test_trg), f\"The src and trg must have the same length got {len(test_src)} and {len(test_trg)}.\""
      ],
      "metadata": {
        "id": "4LqMOxA2NIDd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Counting examples\n",
        "\n",
        "In the following code cell we are going to count examples in each language pair and visualize them using a table."
      ],
      "metadata": {
        "id": "8uKw4HcZUqK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = [\"set\", \"src\", \"trg\"]\n",
        "examples = [\n",
        "    ['training', len(train_src), len(train_trg)],\n",
        "    ['validation' , len(valid_src), len(valid_trg)],\n",
        "    ['testing' , len(test_src), len(test_trg)],\n",
        "]\n",
        "tables.tabulate_data(columns, examples, \"Examples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSXbJRDHNIBW",
        "outputId": "7ef7e03c-6199-49cc-bf9d-11e23873ad8e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-------+-------+\n",
            "| set        |   src |   trg |\n",
            "+------------+-------+-------+\n",
            "| training   | 29001 | 29001 |\n",
            "| validation |  1015 |  1015 |\n",
            "| testing    |  1000 |  1000 |\n",
            "+------------+-------+-------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building Vocabulary\n",
        "Next, we'll build the vocabulary for the `source` (src) and `target` (trg) languages. The vocabulary is used to associate each unique token with an index (an integer). The vocabularies of the `source` and `target` languages are distinct.\n",
        "\n",
        "Using the `min_freq` argument, we only allow tokens that appear at least `2` times to appear in our vocabulary. Tokens that appear only once are converted into an `<unk>` (unknown) token.\n",
        "\n",
        "> It is important to note that our vocabulary should only be built from the `training` set and not the `validation/test` set. This prevents `\"information leakage\"` into our model, giving us artifically inflated `validation/test` scores.\n",
        "\n",
        "Also note that in this notebook we are not going to focus much about text cleaning. We are going to convert sentences to lower case only."
      ],
      "metadata": {
        "id": "JbnP0EM6VcIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counter_src = Counter()\n",
        "for line in train_src:\n",
        "  counter_src.update(tokenize_de(line.lower()))\n",
        "vocabulary_src = vocab.vocab(counter_src, min_freq=5, specials=('-unk-', '-sos-', '-eos-', '-pad-'))\n",
        "\n",
        "counter_trg = Counter()\n",
        "for line in train_trg:\n",
        "  counter_trg.update(tokenize_de(line.lower()))\n",
        "vocabulary_trg = vocab.vocab(counter_trg, min_freq=5, specials=('-unk-', '-sos-', '-eos-', '-pad-'))"
      ],
      "metadata": {
        "id": "BOFgvANUNH-O"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following code cell we are going to get the `string-to-integer` representation of our `src` and `trg` fields."
      ],
      "metadata": {
        "id": "Swjhc1V8SnXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stoi_src = vocabulary_src.get_stoi()\n",
        "stoi_trg = vocabulary_trg.get_stoi()\n",
        "\n",
        "SRC_VOCAB_SIZE = len(stoi_src)\n",
        "TRG_VOCAB_SIZE = len(stoi_trg)"
      ],
      "metadata": {
        "id": "Csd_5BBRNH8S"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SRC and Target Pipelines\n",
        "After our text has been tokenized we need a way of converting those words into numbers because machine leaning models understand numbers not words. That's where we the `src_text_pipeline` and `trg_text_pipeline` functions into play. So these function takes in a sentence and tokenize it then converts each word to a number. Note that the word that does not exists in the vocabulay for either `src` or `trg` will be converted to an `unkown` (`-unk-`) token."
      ],
      "metadata": {
        "id": "MzRQmHw6Xe6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def src_text_pipeline(x: str)->list:\n",
        "  values = list()\n",
        "  tokens = tokenize_de(x.lower())\n",
        "  for token in tokens:\n",
        "    try:\n",
        "      v = stoi_src[token]\n",
        "    except KeyError as e:\n",
        "      v = stoi_trg['-unk-']\n",
        "    values.append(v)\n",
        "  return values\n",
        "\n",
        "def trg_text_pipeline(x: str)->list:\n",
        "  values = list()\n",
        "  tokens = tokenize_de(x.lower())\n",
        "  for token in tokens:\n",
        "    try:\n",
        "      v = stoi_trg[token]\n",
        "    except KeyError as e:\n",
        "      v = stoi_trg['-unk-']\n",
        "    values.append(v)\n",
        "  return values"
      ],
      "metadata": {
        "id": "6Zhq-2_9NH5B"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Translation Dataset\n",
        "\n",
        "In the following code cell we are going to create a `TranslationDataset` class which will inherid from the `torch.utils.data.Dataset` class. This class will take in the `src` and `trg` values and pair them together."
      ],
      "metadata": {
        "id": "vamzzJK5ZEOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, src, trg):\n",
        "    super(TranslationDataset, self).__init__()\n",
        "    self.src = src\n",
        "    self.trg = trg\n",
        "      \n",
        "  def __getitem__(self, index):\n",
        "    return self.src[index], self.trg[index]\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.src)"
      ],
      "metadata": {
        "id": "_fEnlyUEZDi9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### collate_fn of the `DataLoader`\n",
        "Our collate function will be named `tokenize_batch` which takes in the batch of `src` and `trg` pairs then `tokenize`, `numericalize` and then `pad` the to a given `max_len`."
      ],
      "metadata": {
        "id": "5COsTvtUTaEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_batch(batch, max_len=100, padding=\"pre\"):\n",
        "  assert padding==\"pre\" or padding==\"post\", \"the padding can be either pre or post\"\n",
        "  src, trg = [], []\n",
        "  for src_, trg_ in batch:\n",
        "    _src = torch.zeros(max_len, dtype=torch.int32)\n",
        "    _trg = torch.zeros(max_len, dtype=torch.int32)\n",
        "    _src_processed_text = torch.tensor(src_text_pipeline(src_.lower()), dtype=torch.int32)\n",
        "    _trg_processed_text = torch.tensor(trg_text_pipeline(trg_.lower()), dtype=torch.int32)\n",
        "    _src_pos = min(max_len, len(_src_processed_text))\n",
        "    _trg_pos = min(max_len, len(_trg_processed_text))\n",
        "    if padding == \"pre\":\n",
        "      _src[:_src_pos] = _src_processed_text[:_src_pos]\n",
        "      _trg[:_trg_pos] = _trg_processed_text[:_trg_pos]\n",
        "    else:\n",
        "      _src[-_src_pos:] = _src_processed_text[-_src_pos:]\n",
        "      _trg[-_trg_pos:] = _trg_processed_text[-_trg_pos:]\n",
        "    trg.append(_trg.unsqueeze(dim=0))\n",
        "    src.append(_src.unsqueeze(dim=0))\n",
        "  #  the target values must be a LongTensor\n",
        "  return torch.cat(src, dim=0), torch.cat(trg, dim=0).type(torch.LongTensor)"
      ],
      "metadata": {
        "id": "rBepTZbdNH3Z"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating datasets\n",
        "In the following code cell we are going to create datasets for all our `3` sets."
      ],
      "metadata": {
        "id": "UrQYV7ZtUUCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TranslationDataset(train_src, train_trg)\n",
        "test_dataset = TranslationDataset(test_src, test_trg)\n",
        "valid_dataset = TranslationDataset(valid_src, valid_trg)"
      ],
      "metadata": {
        "id": "oInS_T66NH0Y"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking a single example in the `train` set."
      ],
      "metadata": {
        "id": "d0_NQd57UfeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmJVzPTRNHx3",
        "outputId": "df1ca04b-4155-4a9a-a6a1-35e7e7d14d81"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Zwei junge weiÃŸe MÃ¤nner sind im Freien in der NÃ¤he vieler BÃ¼sche.',\n",
              " 'Two young, White males are outside near many bushes.')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DataLoaders\n",
        "\n",
        "In the following code cell we are going to create  dataloaders for our three sets of data. We are going to pass the `tokenize_batch` as our `collate_fn`. We are going to shuffle example in the `train` set only."
      ],
      "metadata": {
        "id": "nUkDJkdAUmdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=tokenize_batch)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=tokenize_batch)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=tokenize_batch)"
      ],
      "metadata": {
        "id": "PqHv_IxzNHuj"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Checking examples in the train_loader."
      ],
      "metadata": {
        "id": "xH9rAhXKVAjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src, trg = next(iter(train_loader))"
      ],
      "metadata": {
        "id": "nlvqqmBSNHsV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILvF8HLbNHpT",
        "outputId": "8d9a9c0d-a267-4a51-8c07-533fe5439c8d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 198, 1220,  153,   20, 2445,  318, 1667, 1823,   15,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0],\n",
              "        [  20,   26,   31,   27,  780,   90,  192,   45, 1700,   15,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0]], dtype=torch.int32)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trg[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIeDIhtZNHnL",
        "outputId": "25d50b3d-9ff5-4982-b721-f7d8c70d9cb5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 111,    9,  211,   21,  574,  204, 2861,    0,   39,  572,   14,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0],\n",
              "        [  21,   29,   34,   21,  850,  751,  233,   21, 1602,   14,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sequence To Sequence Model\n",
        "\n",
        "We are going to build our model in `3` parts. The `econder`, `decoder` and `seq2seq`.\n",
        "\n",
        "### Encoder\n",
        "\n",
        "First, we'll build the encoder. Similar to the previous model, we only use a single layer GRU, however we now use a bidirectional RNN. With a bidirectional RNN, we have two RNNs in each layer. A forward RNN going over the embedded sentence from left to right (shown below in green), and a backward RNN going over the embedded sentence from right to left (teal). All we need to do in code is set bidirectional = True and then pass the embedded sentence to the RNN as before."
      ],
      "metadata": {
        "id": "XO7m3irCV3HA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1mt6XJgaNEv4"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(input_dim, embedding_dim=emb_dim)\n",
        "    self.gru = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "    self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, src): # src = [src len, batch size]\n",
        "    embedded = self.dropout(self.embedding(src)) # embedded = [src len, batch size, emb dim]\n",
        "    outputs, hidden = self.gru(embedded)\n",
        "    \"\"\"\n",
        "    outputs = [src len, batch size, hid dim * num directions]\n",
        "    hidden = [n layers * num directions, batch size, hid dim]\n",
        "\n",
        "    hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "    outputs are always from the last layer\n",
        "\n",
        "    hidden [-2, :, : ] is the last of the forwards RNN \n",
        "    hidden [-1, :, : ] is the last of the backwards RNN\n",
        "\n",
        "    initial decoder hidden is final hidden state of the forwards and backwards \n",
        "    encoder RNNs fed through a linear layer\n",
        "    \"\"\"\n",
        "    hidden = torch.tanh(self.fc(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)))\n",
        "    \"\"\"\n",
        "    outputs = [src len, batch size, enc hid dim * 2]\n",
        "    hidden = [batch size, dec hid dim]\n",
        "    \"\"\"\n",
        "    return outputs, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention Layer\n",
        "\n",
        "You can read more about the implementation of this layer in [this notebook.](https://github.com/CrispenGari/pytorch-python/blob/main/09_NLP/03_Sequence_To_Sequence/03_Neural_Machine_Translation_by_Jointly_Learning_to_Align_and_Translate.ipynb)"
      ],
      "metadata": {
        "id": "7Wcy9r4albnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "    super(Attention, self).__init__()\n",
        "    self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "    self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "\n",
        "  def forward(self, hidden, encoder_outputs):\n",
        "    \"\"\"\n",
        "    hidden = [batch size, dec hid dim]\n",
        "    encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "    \"\"\"\n",
        "    batch_size = encoder_outputs.shape[1]\n",
        "    src_len = encoder_outputs.shape[0]\n",
        "    # repeat decoder hidden state src_len times\n",
        "    hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "    encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "\n",
        "    \"\"\"\n",
        "    hidden = [batch size, src len, dec hid dim]\n",
        "    encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "    \"\"\"\n",
        "    energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) # energy = [batch size, src len, dec hid dim]\n",
        "    attention = self.v(energy).squeeze(2) # attention= [batch size, src len]\n",
        "    return F.softmax(attention, dim=1)"
      ],
      "metadata": {
        "id": "Z2BiLxtMlxKN"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder\n",
        "\n",
        "The decoder contains the attention layer, attention, which takes the previous hidden state. You can read more [here.](https://github.com/CrispenGari/pytorch-python/blob/main/09_NLP/03_Sequence_To_Sequence/03_Neural_Machine_Translation_by_Jointly_Learning_to_Align_and_Translate.ipynb)"
      ],
      "metadata": {
        "id": "8P4rU5fuWqZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.output_dim = output_dim\n",
        "    self.attention = attention\n",
        "\n",
        "    self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "    self.gru = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "    self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "  def forward(self, input, hidden, encoder_outputs):\n",
        "    \"\"\"\n",
        "    input = [batch size]\n",
        "    hidden = [batch size, dec hid dim]\n",
        "    encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "    \"\"\"\n",
        "    input = input.unsqueeze(0) # input = [1, batch size]\n",
        "    embedded = self.dropout(self.embedding(input)) # embedded = [1, batch size, emb dim]\n",
        "    a = self.attention(hidden, encoder_outputs)# a = [batch size, src len]\n",
        "    a = a.unsqueeze(1) # a = [batch size, 1, src len]\n",
        "    encoder_outputs = encoder_outputs.permute(1, 0, 2) # encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "    weighted = torch.bmm(a, encoder_outputs) # weighted = [batch size, 1, enc hid dim * 2]\n",
        "    weighted = weighted.permute(1, 0, 2) # weighted = [1, batch size, enc hid dim * 2]\n",
        "    rnn_input = torch.cat((embedded, weighted), dim = 2) # rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "    output, hidden = self.gru(rnn_input, hidden.unsqueeze(0))\n",
        "    \n",
        "    \"\"\"\n",
        "    output = [seq len, batch size, dec hid dim * n directions]\n",
        "    hidden = [n layers * n directions, batch size, dec hid dim]\n",
        "    \n",
        "    seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
        "    output = [1, batch size, dec hid dim]\n",
        "    hidden = [1, batch size, dec hid dim]\n",
        "    this also means that output == hidden\n",
        "    \"\"\"\n",
        "    assert (output == hidden).all()\n",
        "    embedded = embedded.squeeze(0)\n",
        "    output = output.squeeze(0)\n",
        "    weighted = weighted.squeeze(0)\n",
        "\n",
        "    prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1)) # prediction = [batch size, output dim]\n",
        "    return prediction, hidden.squeeze(0)"
      ],
      "metadata": {
        "id": "LaFQTP3VfyS6"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Seq2Seq (Sequence to Sequence)\n",
        "\n",
        "This is the first model where we don't have to have the encoder RNN and decoder RNN have the same hidden dimensions, however the encoder has to be bidirectional. This requirement can be removed by changing all occurences of enc_dim 2 to enc_dim 2 if encoder_is_bidirectional else enc_dim.\n",
        "\n",
        "This seq2seq encapsulator is similar to the last two. The only difference is that the encoder returns both the final hidden state (which is the final hidden state from both the forward and backward encoder RNNs passed through a linear layer) to be used as the initial hidden state for the decoder, as well as every hidden state (which are the forward and backward hidden states stacked on top of each other). We also need to ensure that hidden and encoder_outputs are passed to the decoder."
      ],
      "metadata": {
        "id": "FI83ZV9gW7jp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder, device):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.device = device\n",
        "        \n",
        "  def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "    \"\"\"\n",
        "    src = [src len, batch size]\n",
        "    trg = [trg len, batch size]\n",
        "    teacher_forcing_ratio is probability to use teacher forcing\n",
        "    e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
        "    \"\"\"\n",
        "    trg_len, batch_size = trg.shape\n",
        "    trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "    # tensor to store decoder outputs\n",
        "    outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "    # encoder_outputs is all hidden states of the input sequence, back and forwards\n",
        "    # hidden is the final forward and backward hidden states, passed through a linear layer\n",
        "    encoder_outputs, hidden = self.encoder(src)     \n",
        "    # first input to the decoder is the <sos> tokens\n",
        "    input = trg[0,:]\n",
        "    for t in range(1, trg_len):\n",
        "      # insert input token embedding, previous hidden state and all encoder hidden states\n",
        "      # receive output tensor (predictions) and new hidden state\n",
        "      output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
        "      \n",
        "      # place predictions in a tensor holding predictions for each token\n",
        "      outputs[t] = output\n",
        "      \n",
        "      # decide if we are going to use teacher forcing or not\n",
        "      teacher_force = random.random() < teacher_forcing_ratio\n",
        "      \n",
        "      # get the highest predicted token from our predictions\n",
        "      top1 = output.argmax(1) \n",
        "      \n",
        "      # if teacher forcing, use actual next token as next input\n",
        "      # if not, use predicted token\n",
        "      input = trg[t] if teacher_force else top1\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "6s5GBR2cf5qq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the `Seq2Seq`\n"
      ],
      "metadata": {
        "id": "6lTf8TlkXaY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = SRC_VOCAB_SIZE\n",
        "OUTPUT_DIM = TRG_VOCAB_SIZE\n",
        "ENC_EMB_DIM = DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = DEC_DROPOUT = 0.5\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baPnQEm-gF8Q",
        "outputId": "79b64030-068c-4fcb-d5e2-b866681eea22"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(3554, 256)\n",
              "    (gru): GRU(256, 512, bidirectional=True)\n",
              "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
              "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
              "    )\n",
              "    (embedding): Embedding(3275, 256)\n",
              "    (gru): GRU(1280, 512)\n",
              "    (fc_out): Linear(in_features=1792, out_features=3275, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Counting model parameters.\n",
        "We are going to use the `model_params` function from `helperfns` to count model parameters."
      ],
      "metadata": {
        "id": "ECIe6GB4Xoyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_params(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdJM5MUagWd-",
        "outputId": "db90aa3e-a656-4252-b413-a63416ea31c7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOTAL MODEL PARAMETERS: \t14,053,579\n",
            "TOTAL TRAINABLE PARAMETERS: \t14,053,579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing the weights\n",
        "here, we will initialize all biases to zero and all weights from $N(0, 0.01)$."
      ],
      "metadata": {
        "id": "Xlq89hwvswai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(m):\n",
        "  for name, param in m.named_parameters():\n",
        "    if 'weight' in name:\n",
        "      nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "    else:\n",
        "      nn.init.constant_(param.data, 0)    \n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVbc6_fOge1Z",
        "outputId": "e1871fbc-1449-4023-d651-118f39ddbfd2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(3554, 256)\n",
              "    (gru): GRU(256, 512, bidirectional=True)\n",
              "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
              "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
              "    )\n",
              "    (embedding): Embedding(3275, 256)\n",
              "    (gru): GRU(1280, 512)\n",
              "    (fc_out): Linear(in_features=1792, out_features=3275, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizer\n",
        "For the optimizer we are going to use the `Adam` optimizer with default parameters."
      ],
      "metadata": {
        "id": "WUPE1wJsZBos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "qJDxezzQgeyv"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Criterion\n",
        "Next, we define our loss function. The `CrossEntropyLoss` function calculates both the log softmax as well as the negative log-likelihood of our predictions.\n",
        "\n",
        "Our loss function calculates the average loss per token, however by passing the index of the `<pad>` token as the `ignore_index` argument we ignore the loss whenever the target token is a padding token."
      ],
      "metadata": {
        "id": "le7qmEbMZQEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRG_PAD_IDX = stoi_trg[\"-pad-\"]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "metadata": {
        "id": "ODrdgxuigev7"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Loop\n",
        "\n",
        "\n",
        "Lets's first create a function that collects all gabages and clears the gabage together with empting cuda cache."
      ],
      "metadata": {
        "id": "RlUORlJ-Z36j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clear_gpu_memory():\n",
        "  torch.cuda.empty_cache()\n",
        "  variables = gc.collect()\n",
        "  del variables"
      ],
      "metadata": {
        "id": "qC4r1AmfYj8a"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "  model.train()\n",
        "  epoch_loss = 0\n",
        "  for i, (src, trg) in enumerate(iterator):\n",
        "    src = src.to(device)\n",
        "    trg = trg.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(src, trg)\n",
        "    # trg = [trg len, batch size]\n",
        "    # output = [trg len, batch size, output dim]\n",
        "    output_dim = output.shape[-1]\n",
        "    output = output[1:].view(-1, output_dim)\n",
        "    trg = trg[1:].view(-1)\n",
        "    # trg = [(trg len - 1) * batch size]\n",
        "    # output = [(trg len - 1) * batch size, output dim]\n",
        "    loss = criterion(output, trg)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "    clear_gpu_memory()\n",
        "  return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "Nj8aeGdzgeuK"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Loop\n"
      ],
      "metadata": {
        "id": "Zv6TJ82padrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "  model.eval()\n",
        "  epoch_loss = 0\n",
        "  with torch.no_grad():\n",
        "    for i, (src, trg) in enumerate(iterator):\n",
        "      src = src.to(device)\n",
        "      trg = trg.to(device)\n",
        "      output = model(src, trg, 0) # turn off teacher forcing\n",
        "      # trg = [trg len, batch size]\n",
        "      # output = [trg len, batch size, output dim]\n",
        "      output_dim = output.shape[-1]\n",
        "      output = output[1:].view(-1, output_dim)\n",
        "      trg = trg[1:].view(-1)\n",
        "      # trg = [(trg len - 1) * batch size]\n",
        "      # output = [(trg len - 1) * batch size, output dim]\n",
        "      loss = criterion(output, trg)\n",
        "      epoch_loss += loss.item()\n",
        "      clear_gpu_memory()\n",
        "  return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "g_F2Wbwjgerf"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running the training loop.\n",
        "During training we are going to visualize our training metrics in tabular form. We are going to save the best model if and only if the previous validation loss is less greater than the current epoch validation loss."
      ],
      "metadata": {
        "id": "kZ9AofNAgdhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "best_valid_loss = float('inf')\n",
        "MODEL_NAME = 'best-model.pt'\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start = time.time()\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_loader, criterion)\n",
        "    title = f\"EPOCH: {epoch+1:02}/{N_EPOCHS:02} {'saving best model...' if valid_loss < best_valid_loss else 'not saving...'}\"\n",
        "    end = time.time()\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), MODEL_NAME)\n",
        "    data = [\n",
        "       [\"Training\", f'{train_loss:.3f}', f'{math.exp(train_loss):7.3f}', f\"{utils.hms_string(end - start)}\" ],\n",
        "       [\"Validation\", f'{valid_loss:.3f}', f'{math.exp(valid_loss):7.3f}', \"\" ],       \n",
        "   ]\n",
        "    columns = [\"CATEGORY\", \"LOSS\", \"PPL\", \"ETA\"]\n",
        "    print(title)\n",
        "    tables.tabulate_data(columns, data, title)"
      ],
      "metadata": {
        "id": "yIttAnWchY6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39fe2d4e-cbc2-4401-ec3f-ba08b0f47cc0"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 01/10 saving best model...\n",
            "+------------+-------+---------+------------+\n",
            "| CATEGORY   |  LOSS |     PPL |        ETA |\n",
            "+------------+-------+---------+------------+\n",
            "| Training   | 0.840 |   2.315 | 0:09:29.16 |\n",
            "| Validation | 0.718 |   2.050 |            |\n",
            "+------------+-------+---------+------------+\n",
            "EPOCH: 02/10 saving best model...\n",
            "+------------+-------+---------+------------+\n",
            "| CATEGORY   |  LOSS |     PPL |        ETA |\n",
            "+------------+-------+---------+------------+\n",
            "| Training   | 0.716 |   2.045 | 0:09:24.43 |\n",
            "| Validation | 0.714 |   2.043 |            |\n",
            "+------------+-------+---------+------------+\n",
            "EPOCH: 03/10 saving best model...\n",
            "+------------+-------+---------+------------+\n",
            "| CATEGORY   |  LOSS |     PPL |        ETA |\n",
            "+------------+-------+---------+------------+\n",
            "| Training   | 0.711 |   2.037 | 0:09:22.23 |\n",
            "| Validation | 0.709 |   2.032 |            |\n",
            "+------------+-------+---------+------------+\n",
            "EPOCH: 04/10 saving best model...\n",
            "+------------+-------+---------+------------+\n",
            "| CATEGORY   |  LOSS |     PPL |        ETA |\n",
            "+------------+-------+---------+------------+\n",
            "| Training   | 0.709 |   2.033 | 0:09:21.51 |\n",
            "| Validation | 0.708 |   2.030 |            |\n",
            "+------------+-------+---------+------------+\n",
            "EPOCH: 05/10 saving best model...\n",
            "+------------+-------+---------+------------+\n",
            "| CATEGORY   |  LOSS |     PPL |        ETA |\n",
            "+------------+-------+---------+------------+\n",
            "| Training   | 0.708 |   2.030 | 0:09:21.55 |\n",
            "| Validation | 0.707 |   2.027 |            |\n",
            "+------------+-------+---------+------------+\n",
            "EPOCH: 06/10 saving best model...\n",
            "+------------+-------+---------+------------+\n",
            "| CATEGORY   |  LOSS |     PPL |        ETA |\n",
            "+------------+-------+---------+------------+\n",
            "| Training   | 0.707 |   2.027 | 0:09:21.67 |\n",
            "| Validation | 0.706 |   2.026 |            |\n",
            "+------------+-------+---------+------------+\n",
            "EPOCH: 07/10 not saving...\n",
            "+------------+-------+---------+------------+\n",
            "| CATEGORY   |  LOSS |     PPL |        ETA |\n",
            "+------------+-------+---------+------------+\n",
            "| Training   | 0.706 |   2.026 | 0:09:21.06 |\n",
            "| Validation | 0.707 |   2.027 |            |\n",
            "+------------+-------+---------+------------+\n",
            "EPOCH: 08/10 saving best model...\n",
            "+------------+-------+---------+------------+\n",
            "| CATEGORY   |  LOSS |     PPL |        ETA |\n",
            "+------------+-------+---------+------------+\n",
            "| Training   | 0.704 |   2.023 | 0:09:20.47 |\n",
            "| Validation | 0.705 |   2.023 |            |\n",
            "+------------+-------+---------+------------+\n",
            "EPOCH: 09/10 saving best model...\n",
            "+------------+-------+---------+------------+\n",
            "| CATEGORY   |  LOSS |     PPL |        ETA |\n",
            "+------------+-------+---------+------------+\n",
            "| Training   | 0.704 |   2.021 | 0:09:21.76 |\n",
            "| Validation | 0.704 |   2.022 |            |\n",
            "+------------+-------+---------+------------+\n",
            "EPOCH: 10/10 saving best model...\n",
            "+------------+-------+---------+------------+\n",
            "| CATEGORY   |  LOSS |     PPL |        ETA |\n",
            "+------------+-------+---------+------------+\n",
            "| Training   | 0.702 |   2.019 | 0:09:21.65 |\n",
            "| Validation | 0.703 |   2.020 |            |\n",
            "+------------+-------+---------+------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the Best Model\n",
        "\n",
        "In the following code cell we are going to evaluate the best model."
      ],
      "metadata": {
        "id": "nmGj2TFZcdO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = [\"Set\", \"Loss\", \"PPL\", \"ETA (time)\"]\n",
        "model.load_state_dict(torch.load(MODEL_NAME))\n",
        "test_loss= evaluate(model, test_loader, criterion)\n",
        "title = \"Model Evaluation Summary\"\n",
        "data_rows = [[\"Test\", f'{test_loss:.3f}', f'{math.exp(test_loss):7.3f}', \"\"]]\n",
        "\n",
        "tables.tabulate_data(column_names, data_rows, title)"
      ],
      "metadata": {
        "id": "ncbUrUC3hZgG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af008a67-f764-45d7-e756-983270504cd2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+---------+------------+\n",
            "| Set  |  Loss |     PPL | ETA (time) |\n",
            "+------+-------+---------+------------+\n",
            "| Test | 0.705 |   2.023 |            |\n",
            "+------+-------+---------+------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We've improved on the previous model, but this came at the cost of doubling the training time.\n",
        "\n",
        "In the next notebook, we'll be using the same architecture but using a few tricks that are applicable to all RNN architectures - **packed padded sequences** and **masking**. We'll also implement code which will allow us to look at what words in the input the RNN is paying attention to when decoding the output."
      ],
      "metadata": {
        "id": "JzLCGynMc5eO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hwmFRXpeidME"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}