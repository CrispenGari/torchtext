{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PufgHqGWNPxt"
      },
      "source": [
        "### Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation.\n",
        "\n",
        "In this notebook we are going to implement the model from [this](https://arxiv.org/abs/1406.1078) paper. In the previous notebook we created a `Seq2Seq` model.\n",
        "\n",
        "One downside of the previous model is that the decoder is trying to cram lots of information into the hidden states. Whilst decoding, the hidden state will need to contain information about the whole of the source sequence, as well as all of the tokens have been decoded so far. By alleviating some of this information compression, we can create a better model!\n",
        "\n",
        "We'll also be using a `GRU` (Gated Recurrent Unit) instead of an `LSTM` (Long Short-Term Memory). Why? Mainly because that's what they did in the paper (this paper also introduced `GRUs`) and also because we used `LSTMs` last time. Both `GRU` and `LSTM` are pretty much the same as they differ from regular `RNNs`.\n",
        "\n",
        "> This notebook is nothing but a modified version of my previous notebooks on `seq2seq` machine translation model. The only difference is that in this notebook we are going to use the `torchtext` api instead of the `torchtext.lergacy`. The following notebooks are used as refences to this notebook. There you will find a deep understanding on how `Seq2Seq` models work.\n",
        "\n",
        "1. [02__Learning_Phrase_Representations_using_RNN_Encoder_Decoder_for_Statistical_Machine_Translation.ipynb](https://github.com/CrispenGari/pytorch-python/blob/main/09_NLP/03_Sequence_To_Sequence/02__Learning_Phrase_Representations_using_RNN_Encoder_Decoder_for_Statistical_Machine_Translation.ipynb)\n",
        "\n",
        "\n",
        "### Installation of Packages\n",
        "In the following code cell we are going to install the packages that we are going to use in this notebook which are `helperfns` and `torchdata`. The `helperfns` package allows us to get some machine learning helper function which we are also going to use in this notebook as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jOHX7hrENIkU"
      },
      "outputs": [],
      "source": [
        "!pip install helperfns torchdata -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z_Ck4gdOv3K"
      },
      "source": [
        "### Imports\n",
        "\n",
        "In the following code cell we are going to import the packages that are going to be used in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rafqZG3wNIhY",
        "outputId": "d9475b24-099c-4dcb-f7ba-13dc7215feb8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('0.13.1', '1.12.1+cu113')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "from torch import nn\n",
        "from torch.nn  import functional as F\n",
        "from torchtext import data, datasets\n",
        "from collections import Counter\n",
        "from torchtext import vocab\n",
        "from helperfns import tables, visualization, utils\n",
        "from helperfns.torch.models import model_params\n",
        "\n",
        "import spacy\n",
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torchtext\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "torchtext.__version__, torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okz1JiI6P5xV"
      },
      "source": [
        "### Seed\n",
        "In the following code cell we are going to set up the `SEED` for reproducivity in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8klW-wN0NId_"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deteministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu0y1XQ4QIGS"
      },
      "source": [
        "### Device\n",
        "\n",
        "In the following code cell we are going to declare a `device` variable that will allow us to make use of `GPU` if available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyGo-myfNIb-",
        "outputId": "ad5c1866-9d99-48ce-855a-d23871d679f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWDaqUDIQf4E"
      },
      "source": [
        "### Tokenizer Models\n",
        "\n",
        "For the tokenizer we are going to make use of the `spacy` language models to tokenize sentences for each language. So first we will need to download the `en_core_web_sm` and the `de_core_news_sm`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGD13NUYNIYb",
        "outputId": "c7334fb5-c361-4c54-c4ec-e1bf8f481a06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "spacy.cli.download('de_core_news_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gXc95YjQNIWm"
      },
      "outputs": [],
      "source": [
        "spacy_de = spacy.load('de_core_news_sm')\n",
        "spacy_en = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMvQzFTYRfZz"
      },
      "source": [
        "In the following code cell we are going to create our tokenizers functions that takes in a sentence in english and tokenize it.\n",
        "\n",
        "> Previously we reversed the source (German) sentence, however in the paper we are implementing they don't do this, so neither will we."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Da_roQ2ONITL"
      },
      "outputs": [],
      "source": [
        "def tokenize_de(sent:str)->list:\n",
        "  return [tok.text for tok in spacy_de.tokenizer(sent)]\n",
        "\n",
        "def tokenize_en(sent:str)->list:\n",
        "  return [tok.text for tok in spacy_en.tokenizer(sent)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81jcHtDkSGTr"
      },
      "source": [
        "Testing our tokenizer functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaE7za2mNIRX",
        "outputId": "8d01de0a-73ea-4671-d510-0c240e20a876"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Kannst', 'du', 'mir', 'helfen', '?']"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenize_de(\"Kannst du mir helfen?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5RROtSYNIOd",
        "outputId": "855bffb0-39cd-463a-af99-b123bc16f44f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Can', 'you', 'help', 'me', '?']"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenize_en(\"Can you help me?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYv-xr3cSe7I"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The dataset we'll be using is the [Multi30k](https://pytorch.org/text/stable/datasets.html#multi30k) dataset. This is a dataset with `~30,000` parallel English, German and French sentences, each with `~12` words per sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "K6DayGR2NIMD"
      },
      "outputs": [],
      "source": [
        "train_iter, valid_iter, test_iter = datasets.Multi30k(\n",
        "     root = '.data', split = ('train', 'valid', 'test'), \n",
        "    language_pair= ('de', 'en')\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAmFsNzsSQoW"
      },
      "source": [
        "Checking as single train example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKuNV_MaNIIo",
        "outputId": "a718cffa-acc0-43df-e8f5-95b23ee6d386"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.',\n",
              " 'Two young, White males are outside near many bushes.')"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(iter(train_iter))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCe9GrbgTZdC"
      },
      "source": [
        "### Getting `src` and `trg`\n",
        "Our `src` field will be the `de` sentence and the `trg` field will the the `en` sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "-fOFrxOTNIGi"
      },
      "outputs": [],
      "source": [
        "train_src = []\n",
        "train_trg = []\n",
        "valid_src = []\n",
        "valid_trg = []\n",
        "test_src = []\n",
        "test_trg = []\n",
        "\n",
        "for (src, trg) in train_iter:\n",
        "  train_src.append(src)\n",
        "  train_trg.append(trg)\n",
        "\n",
        "for (src, trg) in test_iter:\n",
        "  test_src.append(src)\n",
        "  test_trg.append(trg)\n",
        "\n",
        "for (src, trg) in valid_iter:\n",
        "  valid_src.append(src)\n",
        "  valid_trg.append(trg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSIeQpwNSXGG"
      },
      "source": [
        "Checking if the `src` and the `trg` has the same length for all sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "4LqMOxA2NIDd"
      },
      "outputs": [],
      "source": [
        "assert len(train_src) == len(train_trg), f\"The src and trg must have the same length got {len(train_src)} and {len(train_trg)}.\"\n",
        "assert len(valid_src) == len(valid_trg), f\"The src and trg must have the same length got {len(valid_src)} and {len(valid_trg)}.\"\n",
        "assert len(test_src) == len(test_trg), f\"The src and trg must have the same length got {len(test_src)} and {len(test_trg)}.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uKw4HcZUqK-"
      },
      "source": [
        "### Counting examples\n",
        "\n",
        "In the following code cell we are going to count examples in each language pair and visualize them using a table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSXbJRDHNIBW",
        "outputId": "8ae55e64-40d1-4d0c-805c-f0973ad08fed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------+\n",
            "|          Examples          |\n",
            "+------------+-------+-------+\n",
            "|    set     |  src  |  trg  |\n",
            "+------------+-------+-------+\n",
            "| training   | 29001 | 29001 |\n",
            "| validation |  1015 |  1015 |\n",
            "| testing    |  1000 |  1000 |\n",
            "+------------+-------+-------+\n"
          ]
        }
      ],
      "source": [
        "columns = [\"set\", \"src\", \"trg\"]\n",
        "examples = [\n",
        "    ['training', len(train_src), len(train_trg)],\n",
        "    ['validation' , len(valid_src), len(valid_trg)],\n",
        "    ['testing' , len(test_src), len(test_trg)],\n",
        "]\n",
        "tables.tabulate_data(columns, examples, \"Examples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbnP0EM6VcIq"
      },
      "source": [
        "### Building Vocabulary\n",
        "Next, we'll build the vocabulary for the `source` (src) and `target` (trg) languages. The vocabulary is used to associate each unique token with an index (an integer). The vocabularies of the `source` and `target` languages are distinct.\n",
        "\n",
        "Using the `min_freq` argument, we only allow tokens that appear at least `2` times to appear in our vocabulary. Tokens that appear only once are converted into an `<unk>` (unknown) token.\n",
        "\n",
        "> It is important to note that our vocabulary should only be built from the `training` set and not the `validation/test` set. This prevents `\"information leakage\"` into our model, giving us artifically inflated `validation/test` scores.\n",
        "\n",
        "Also note that in this notebook we are not going to focus much about text cleaning. We are going to convert sentences to lower case only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "BOFgvANUNH-O"
      },
      "outputs": [],
      "source": [
        "counter_src = Counter()\n",
        "for line in train_src:\n",
        "  counter_src.update(tokenize_de(line.lower()))\n",
        "vocabulary_src = vocab.vocab(counter_src, min_freq=5, specials=('<unk>', '<sos>', '<eos>', '<pad>'))\n",
        "\n",
        "counter_trg = Counter()\n",
        "for line in train_trg:\n",
        "  counter_trg.update(tokenize_de(line.lower()))\n",
        "vocabulary_trg = vocab.vocab(counter_trg, min_freq=5, specials=('<unk>', '<sos>', '<eos>', '<pad>'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Swjhc1V8SnXj"
      },
      "source": [
        "In the following code cell we are going to get the `string-to-integer` representation of our `src` and `trg` fields."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Csd_5BBRNH8S"
      },
      "outputs": [],
      "source": [
        "stoi_src = vocabulary_src.get_stoi()\n",
        "stoi_trg = vocabulary_trg.get_stoi()\n",
        "\n",
        "SRC_VOCAB_SIZE = len(stoi_src)\n",
        "TRG_VOCAB_SIZE = len(stoi_trg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzRQmHw6Xe6M"
      },
      "source": [
        "### SRC and Target Pipelines\n",
        "After our text has been tokenized we need a way of converting those words into numbers because machine leaning models understand numbers not words. That's where we the `src_text_pipeline` and `trg_text_pipeline` functions into play. So these function takes in a sentence and tokenize it then converts each word to a number. Note that the word that does not exists in the vocabulay for either `src` or `trg` will be converted to an `unkown` (`<unk>`) token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "6Zhq-2_9NH5B"
      },
      "outputs": [],
      "source": [
        "def src_text_pipeline(x: str)->list:\n",
        "  values = list()\n",
        "  tokens = tokenize_de(x.lower())\n",
        "  for token in tokens:\n",
        "    try:\n",
        "      v = stoi_src[token]\n",
        "    except KeyError as e:\n",
        "      v = stoi_trg['<unk>']\n",
        "    values.append(v)\n",
        "  return values\n",
        "\n",
        "def trg_text_pipeline(x: str)->list:\n",
        "  values = list()\n",
        "  tokens = tokenize_de(x.lower())\n",
        "  for token in tokens:\n",
        "    try:\n",
        "      v = stoi_trg[token]\n",
        "    except KeyError as e:\n",
        "      v = stoi_trg['<unk>']\n",
        "    values.append(v)\n",
        "  return values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vamzzJK5ZEOD"
      },
      "source": [
        "#### Translation Dataset\n",
        "\n",
        "In the following code cell we are going to create a `TranslationDataset` class which will inherid from the `torch.utils.data.Dataset` class. This class will take in the `src` and `trg` values and pair them together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "_fEnlyUEZDi9"
      },
      "outputs": [],
      "source": [
        "class TranslationDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, src, trg):\n",
        "    super(TranslationDataset, self).__init__()\n",
        "    self.src = src\n",
        "    self.trg = trg\n",
        "      \n",
        "  def __getitem__(self, index):\n",
        "    return self.src[index], self.trg[index]\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.src)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5COsTvtUTaEl"
      },
      "source": [
        "### collate_fn of the `DataLoader`\n",
        "Our collate function will be named `tokenize_batch` which takes in the batch of `src` and `trg` pairs then `tokenize`, `numericalize` and then `pad` the to a given `max_len`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "rBepTZbdNH3Z"
      },
      "outputs": [],
      "source": [
        "def tokenize_batch(batch, max_len=100, padding=\"pre\"):\n",
        "  assert padding==\"pre\" or padding==\"post\", \"the padding can be either pre or post\"\n",
        "  src, trg = [], []\n",
        "  for src_, trg_ in batch:\n",
        "    _src = torch.zeros(max_len, dtype=torch.int32)\n",
        "    _trg = torch.zeros(max_len, dtype=torch.int32)\n",
        "    _src_processed_text = torch.tensor(src_text_pipeline(src_.lower()), dtype=torch.int32)\n",
        "    _trg_processed_text = torch.tensor(trg_text_pipeline(trg_.lower()), dtype=torch.int32)\n",
        "    _src_pos = min(max_len, len(_src_processed_text))\n",
        "    _trg_pos = min(max_len, len(_trg_processed_text))\n",
        "    if padding == \"pre\":\n",
        "      _src[:_src_pos] = _src_processed_text[:_src_pos]\n",
        "      _trg[:_trg_pos] = _trg_processed_text[:_trg_pos]\n",
        "    else:\n",
        "      _src[-_src_pos:] = _src_processed_text[-_src_pos:]\n",
        "      _trg[-_trg_pos:] = _trg_processed_text[-_trg_pos:]\n",
        "    trg.append(_trg.unsqueeze(dim=0))\n",
        "    src.append(_src.unsqueeze(dim=0))\n",
        "  #  the target values must be a LongTensor\n",
        "  return torch.cat(src, dim=0), torch.cat(trg, dim=0).type(torch.LongTensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrQYV7ZtUUCr"
      },
      "source": [
        "### Creating datasets\n",
        "In the following code cell we are going to create datasets for all our `3` sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "oInS_T66NH0Y"
      },
      "outputs": [],
      "source": [
        "train_dataset = TranslationDataset(train_src, train_trg)\n",
        "test_dataset = TranslationDataset(test_src, test_trg)\n",
        "valid_dataset = TranslationDataset(valid_src, valid_trg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0_NQd57UfeG"
      },
      "source": [
        "Checking a single example in the `train` set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmJVzPTRNHx3",
        "outputId": "84d5848f-678f-46ea-d08e-41b099b8d92a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.',\n",
              " 'Two young, White males are outside near many bushes.')"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUkDJkdAUmdG"
      },
      "source": [
        "### DataLoaders\n",
        "\n",
        "In the following code cell we are going to create  dataloaders for our three sets of data. We are going to pass the `tokenize_batch` as our `collate_fn`. We are going to shuffle example in the `train` set only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "PqHv_IxzNHuj"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=tokenize_batch)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=tokenize_batch)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=tokenize_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH9rAhXKVAjy"
      },
      "source": [
        "#### Checking examples in the train_loader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "nlvqqmBSNHsV"
      },
      "outputs": [],
      "source": [
        "src, trg = iter(train_loader).next()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILvF8HLbNHpT",
        "outputId": "1fc9d50e-e94b-4ea3-ab47-4f6fea099a95"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[  45,   61,   17, 2714,   29, 3011,  175,   45, 1138,   54,   47,   48,\n",
              "           58,  209,   32,  243,   61,   99,   15,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0],\n",
              "        [  20,    5, 1355,   20,    0,   47,   20,  314,  344,  175,    0,   11,\n",
              "          332,  995,   15,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0]], dtype=torch.int32)"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "src[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIeDIhtZNHnL",
        "outputId": "1f354b30-ece8-48eb-ab0c-043d531b0dab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[  21,   59,   80,   21,  389,   83,   32, 3181,   21, 1206,   46,  101,\n",
              "           60,  286,   59,   14,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0],\n",
              "        [  21,    5,  220, 1422,   21,  414,   70,    0,    6,   60,  341,    0,\n",
              "           17,   49, 1039,   14,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0]])"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trg[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO7m3irCV3HA"
      },
      "source": [
        "### Sequence To Sequence Model\n",
        "\n",
        "We are going to build our model in `3` parts. The `econder`, `decoder` and `seq2seq`. You can refer from [this notebook](https://github.com/CrispenGari/pytorch-python/blob/main/09_NLP/03_Sequence_To_Sequence/01_Sequence_To_Sequence_Introduction.ipynb).\n",
        "\n",
        "\n",
        "### Encoder\n",
        "\n",
        "The encoder is similar to the previous one, with the multi-layer `LSTM` swapped for a single-layer `GRU`. We also don't pass the dropout as an argument to the `GRU` as that dropout is used between each layer of a multi-layered `RNN`. As we only have a single layer, PyTorch will display a warning if we try and use pass a dropout value to it.\n",
        "\n",
        "Another thing to note about the GRU is that it only requires and returns a hidden state, there is no cell state like in the `LSTM`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "1mt6XJgaNEv4"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.hid_dim = hid_dim\n",
        "    self.embedding = nn.Embedding(input_dim, embedding_dim=emb_dim)\n",
        "    \"\"\"\n",
        "    No Dropout (GRU) since we have **one** layer.\n",
        "    \"\"\"\n",
        "    self.gru = nn.GRU(emb_dim, hid_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, src):\n",
        "    # src = [src len, batch size]\n",
        "    embedded = self.dropout(self.embedding(src))\n",
        "    # embedded = [src len, batch size, emb dim]\n",
        "    ouputs, h_0 = self.gru(embedded) # no cell state since it is a GRU not LSTM\n",
        "    \"\"\"\n",
        "    outputs = [src len, batch size, hid dim * n directions]\n",
        "    hidden (h_0) = [n_layers * n_directions, batch size, hid dim]\n",
        "    ** outputs are always from the top hidden layer\n",
        "    \"\"\"\n",
        "    return h_0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P4rU5fuWqZr"
      },
      "source": [
        "### Decoder\n",
        "\n",
        "The decoder is where the implementation differs significantly from the previous model and we alleviate some of the information compression. You can read more [here.](https://github.com/CrispenGari/pytorch-python/blob/main/09_NLP/03_Sequence_To_Sequence/02__Learning_Phrase_Representations_using_RNN_Encoder_Decoder_for_Statistical_Machine_Translation.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "LaFQTP3VfyS6"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.hid_dim = hid_dim\n",
        "    self.output_dim = output_dim\n",
        "\n",
        "    self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "    self.gru = nn.GRU(emb_dim + hid_dim, hid_dim)\n",
        "    self.fc = nn.Linear(emb_dim + hid_dim * 2, output_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, input, hidden, context):\n",
        "    \"\"\"     \n",
        "    input = [batch size]\n",
        "    hidden, (h_0) = [n_layers * n_directions, batch_size, hid_dim]\n",
        "    context = [n_layers * n_directions, batch_size, hid_dim]\n",
        "\n",
        "    n_layers and n_directions in the decoder will both always be 1, therefore:\n",
        "    hidden (h_0) = [1, batch_size, hid_dim]\n",
        "    context = [1, batch_size, hid_dim]\n",
        "    \"\"\"\n",
        "    input = input.unsqueeze(0)  # nput = [1, batch size]\n",
        "    embedded = self.dropout(self.embedding(input)) # embedded = [1, batch_size, emb_dim]\n",
        "    emb_con = torch.cat((embedded, context), dim = 2) # emb_con = [1, batch size, emb dim + hid dim]\n",
        "\n",
        "    output, h_0 = self.gru(emb_con, hidden)\n",
        "    \"\"\"      \n",
        "    output = [seq_len, batch_size, hid dim * n_directions]\n",
        "    hidden (h_0) = [n_layers * n_directions, batch_size, hid_dim]\n",
        "\n",
        "    seq_len, n_layers and n_directions will always be 1 in the decoder, therefore:\n",
        "    output = [1, batch_size, hid_dim]\n",
        "    hidden (h_0) = [1, batch_size, hid_dim]\n",
        "    \"\"\"\n",
        "    output = torch.cat((embedded.squeeze(0), h_0.squeeze(0), context.squeeze(0)), \n",
        "                           dim = 1) # output = [batch size, emb dim + hid dim * 2]\n",
        "    prediction = self.fc(output) # prediction = [batch size, output dim]\n",
        "    return prediction, h_0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI83ZV9gW7jp"
      },
      "source": [
        "###Seq2Seq (Sequence to Sequence)\n",
        "For the final part of the implemenetation, we'll implement the `seq2seq` model. This will handle:\n",
        "\n",
        "* the outputs tensor is created to hold all predictions, \n",
        "* the source sequence, is fed into the encoder to receive a context vector\n",
        "* the initial decoder hidden state is set to be the context vector, \n",
        "* we use a batch of `<sos> ` tokens as the first input, \n",
        "* we then decode within a loop:\n",
        "  * inserting the input token, previous hidden state, , and the context vector, , into the decoder\n",
        "* receiving a prediction, , and a new hidden state, \n",
        "* we then decide if we are going to teacher force or not, setting the next input as appropriate (either the ground truth next token in the target sequence or the highest predicted next token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "6s5GBR2cf5qq"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        assert encoder.hid_dim == decoder.hid_dim, \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \"\"\"\n",
        "        src = [src_len, batch_size]\n",
        "        trg = [trg_len, batch_size]\n",
        "        teacher_forcing_ratio is probability to use teacher forcing\n",
        "        e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "        \"\"\"\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        # tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        # last hidden state of the encoder is the context\n",
        "        context = self.encoder(src)\n",
        "        \n",
        "        # context also used as the initial hidden state of the decoder\n",
        "        hidden = context\n",
        "        \n",
        "        # first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        for t in range(1, trg_len):\n",
        "            # insert input token embedding, previous hidden state and the context state\n",
        "            # receive output tensor (predictions) and new hidden state\n",
        "            output, hidden = self.decoder(input, hidden, context)\n",
        "            # place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            # decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            # get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            # if teacher forcing, use actual next token as next input\n",
        "            # if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lTf8TlkXaY4"
      },
      "source": [
        "### Training the `Seq2Seq`\n",
        "Now we have our model implemented, we can begin training it.\n",
        "\n",
        "First, we'll initialize our model. As mentioned before, the `input` and `output` dimensions are defined by the size of the vocabulary. The embedding dimesions and dropout for the encoder and decoder can be different, but the number of layers and the size of the hidden/cell states must be the same.\n",
        "\n",
        "We then define the encoder, decoder and then our `Seq2Seq` model, which we place on the device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baPnQEm-gF8Q",
        "outputId": "669f86f7-dcdc-49a4-eca8-345804b3f1fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(3554, 256)\n",
              "    (gru): GRU(256, 512)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(3275, 256)\n",
              "    (gru): GRU(768, 512)\n",
              "    (fc): Linear(in_features=1280, out_features=3275, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "INPUT_DIM = SRC_VOCAB_SIZE\n",
        "OUTPUT_DIM = TRG_VOCAB_SIZE\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT)\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECIe6GB4Xoyu"
      },
      "source": [
        "### Counting model parameters.\n",
        "We are going to use the `model_params` function from `helperfns` to count model parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdJM5MUagWd-",
        "outputId": "9a122b5e-b708-4f44-b769-48634ef6b55d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TOTAL MODEL PARAMETERS: \t9095371\n",
            "TOTAL TRAINABLE PARAMETERS: \t9095371\n"
          ]
        }
      ],
      "source": [
        "model_params(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f_hhdNOYvHD"
      },
      "source": [
        "### Initializing weights to the model\n",
        "\n",
        "Next, we initialize our parameters. The paper states the parameters are initialized from a normal distribution with a mean of `0` and a standard deviation of `0.01`, i.e. $N(0, 0.01)$ .\n",
        "\n",
        "It also states we should initialize the recurrent parameters to a special initialization, however to keep things simple we'll also initialize them to $N(0, 0.01)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVbc6_fOge1Z",
        "outputId": "e5f14d6e-ff84-4fe7-f80e-21a5a59746b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(3554, 256)\n",
              "    (gru): GRU(256, 512)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(3275, 256)\n",
              "    (gru): GRU(768, 512)\n",
              "    (fc): Linear(in_features=1280, out_features=3275, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        \n",
        "model.apply(init_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUPE1wJsZBos"
      },
      "source": [
        "### Optimizer\n",
        "For the optimizer we are going to use the `Adam` optimizer with default parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "qJDxezzQgeyv"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le7qmEbMZQEl"
      },
      "source": [
        "### Criterion\n",
        "Next, we define our loss function. The `CrossEntropyLoss` function calculates both the log softmax as well as the negative log-likelihood of our predictions.\n",
        "\n",
        "Our loss function calculates the average loss per token, however by passing the index of the `<pad>` token as the `ignore_index` argument we ignore the loss whenever the target token is a padding token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "ODrdgxuigev7"
      },
      "outputs": [],
      "source": [
        "TRG_PAD_IDX = stoi_trg[\"<pad>\"]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlUORlJ-Z36j"
      },
      "source": [
        "### Train Loop\n",
        "\n",
        "First, we'll set the model into \"training mode\" with `model.train()`. This will turn on dropout (and batch normalization, which we aren't using) and then iterate through our data iterator.\n",
        "\n",
        "At each iteration:\n",
        "\n",
        "* get the source and target sentences from the batch,  and put them to the `device` \n",
        "* zero the gradients calculated from the last batch\n",
        "* feed the source and target into the model to get the output, \n",
        "* as the loss function only works on 2d inputs with 1d targets we need to flatten each of them with `.view`\n",
        "  * we slice off the first column of the output and target tensors as mentioned above\n",
        "* calculate the gradients with `loss.backward()`\n",
        "* clip the gradients to prevent them from exploding (a common issue in RNNs)\n",
        "* update the parameters of our model by doing an optimizer step\n",
        "* sum the loss value to a running total\n",
        "* Finally, we return the loss that is averaged over all batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "Nj8aeGdzgeuK"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "  model.train()\n",
        "  epoch_loss = 0\n",
        "  for i, (src, trg) in enumerate(iterator):\n",
        "    src = src.to(device)\n",
        "    trg = trg.type(torch.LongTensor).to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(src, trg)\n",
        "    \"\"\"\n",
        "    trg = [trg len, batch size]\n",
        "    output = [trg len, batch size, output dim]\n",
        "    \"\"\"\n",
        "    output_dim = output.shape[-1]\n",
        "    output = output[1:].view(-1, output_dim)\n",
        "    trg = trg[1:].view(-1)\n",
        "\n",
        "    \"\"\"\n",
        "    trg = [(trg len - 1) * batch size]\n",
        "    output = [(trg len - 1) * batch size, output dim]\n",
        "    \"\"\"\n",
        "    loss = criterion(output, trg)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv6TJ82padrH"
      },
      "source": [
        "### Evaluation Loop\n",
        "Our evaluation loop is similar to our training loop, however as we aren't updating any parameters we don't need to pass an `optimizer` or a `clip` value.\n",
        "\n",
        "We must remember to set the model to evaluation mode with `model.eval()`. This will turn off dropout (and batch normalization, if used).\n",
        "\n",
        "We use the with `torch.no_grad()` block to ensure no gradients are calculated within the block. This reduces memory consumption and speeds things up.\n",
        "\n",
        "The iteration loop is similar (without the parameter updates), however we must ensure we turn teacher forcing off for evaluation. This will cause the model to only use it's own predictions to make further predictions within a sentence, which mirrors how it would be used in deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "g_F2Wbwjgerf"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "  model.eval()\n",
        "  epoch_loss = 0\n",
        "  with torch.no_grad():\n",
        "    for i, (src, trg) in enumerate(iterator):\n",
        "      src = src.to(device)\n",
        "      trg = trg.type(torch.LongTensor).to(device)\n",
        "      output = model(src, trg, 0) #turn off teacher forcing\n",
        "      \"\"\"\n",
        "      trg = [trg len, batch size]\n",
        "      output = [trg len, batch size, output dim]\n",
        "      \"\"\"\n",
        "      output_dim = output.shape[-1]\n",
        "      output = output[1:].view(-1, output_dim)\n",
        "      trg = trg[1:].view(-1)\n",
        "      \"\"\"\n",
        "      trg = [(trg len - 1) * batch size]\n",
        "      output = [(trg len - 1) * batch size, output dim]\n",
        "      \"\"\"\n",
        "      loss = criterion(output, trg)\n",
        "      epoch_loss += loss.item()\n",
        "  return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ9AofNAgdhP"
      },
      "source": [
        "### Running the training loop.\n",
        "During training we are going to visualize our training metrics in tabular form. We are going to save the best model if and only if the previous validation loss is less greater than the current epoch validation loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIttAnWchY6f",
        "outputId": "628799b9-0ab4-41e1-dc43-a04fbaeb095c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------------------------+\n",
            "|     EPOCH: 01/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "|  CATEGORY  |  LOSS |   PPL    |    ETA     |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.036 |    1.036 | 0:00:01.95 |\n",
            "| Validation | 8.021 | 3043.698 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 02/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "|  CATEGORY  |  LOSS |   PPL    |    ETA     |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.035 |    1.036 | 0:00:01.83 |\n",
            "| Validation | 7.901 | 2700.634 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 03/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "|  CATEGORY  |  LOSS |   PPL    |    ETA     |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.035 |    1.035 | 0:00:01.84 |\n",
            "| Validation | 7.631 | 2060.750 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+-------------------------------------------+\n",
            "|     EPOCH: 04/10 saving best model...     |\n",
            "+------------+-------+---------+------------+\n",
            "|  CATEGORY  |  LOSS |   PPL   |    ETA     |\n",
            "+------------+-------+---------+------------+\n",
            "| Training   | 0.034 |   1.034 | 0:00:01.84 |\n",
            "| Validation | 6.735 | 841.462 |            |\n",
            "+------------+-------+---------+------------+\n",
            "+-------------------------------------------+\n",
            "|     EPOCH: 05/10 saving best model...     |\n",
            "+------------+-------+---------+------------+\n",
            "|  CATEGORY  |  LOSS |   PPL   |    ETA     |\n",
            "+------------+-------+---------+------------+\n",
            "| Training   | 0.030 |   1.030 | 0:00:01.83 |\n",
            "| Validation | 2.982 |  19.721 |            |\n",
            "+------------+-------+---------+------------+\n",
            "+-------------------------------------------+\n",
            "|     EPOCH: 06/10 saving best model...     |\n",
            "+------------+-------+---------+------------+\n",
            "|  CATEGORY  |  LOSS |   PPL   |    ETA     |\n",
            "+------------+-------+---------+------------+\n",
            "| Training   | 0.013 |   1.013 | 0:00:01.83 |\n",
            "| Validation | 1.156 |   3.176 |            |\n",
            "+------------+-------+---------+------------+\n",
            "+-------------------------------------------+\n",
            "|         EPOCH: 07/10 not saving...        |\n",
            "+------------+-------+---------+------------+\n",
            "|  CATEGORY  |  LOSS |   PPL   |    ETA     |\n",
            "+------------+-------+---------+------------+\n",
            "| Training   | 0.005 |   1.005 | 0:00:01.83 |\n",
            "| Validation | 1.197 |   3.310 |            |\n",
            "+------------+-------+---------+------------+\n",
            "+-------------------------------------------+\n",
            "|         EPOCH: 08/10 not saving...        |\n",
            "+------------+-------+---------+------------+\n",
            "|  CATEGORY  |  LOSS |   PPL   |    ETA     |\n",
            "+------------+-------+---------+------------+\n",
            "| Training   | 0.006 |   1.006 | 0:00:01.84 |\n",
            "| Validation | 1.188 |   3.280 |            |\n",
            "+------------+-------+---------+------------+\n",
            "+-------------------------------------------+\n",
            "|         EPOCH: 09/10 not saving...        |\n",
            "+------------+-------+---------+------------+\n",
            "|  CATEGORY  |  LOSS |   PPL   |    ETA     |\n",
            "+------------+-------+---------+------------+\n",
            "| Training   | 0.005 |   1.005 | 0:00:01.83 |\n",
            "| Validation | 1.182 |   3.261 |            |\n",
            "+------------+-------+---------+------------+\n",
            "+-------------------------------------------+\n",
            "|         EPOCH: 10/10 not saving...        |\n",
            "+------------+-------+---------+------------+\n",
            "|  CATEGORY  |  LOSS |   PPL   |    ETA     |\n",
            "+------------+-------+---------+------------+\n",
            "| Training   | 0.006 |   1.006 | 0:00:01.84 |\n",
            "| Validation | 1.157 |   3.181 |            |\n",
            "+------------+-------+---------+------------+\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "best_valid_loss = float('inf')\n",
        "MODEL_NAME = 'best-model.pt'\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start = time.time()\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_loader, criterion)\n",
        "    title = f\"EPOCH: {epoch+1:02}/{N_EPOCHS:02} {'saving best model...' if valid_loss < best_valid_loss else 'not saving...'}\"\n",
        "    end = time.time()\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), MODEL_NAME)\n",
        "    data = [\n",
        "       [\"Training\", f'{train_loss:.3f}', f'{math.exp(train_loss):7.3f}', f\"{utils.hms_string(end - start)}\" ],\n",
        "       [\"Validation\", f'{valid_loss:.3f}', f'{math.exp(valid_loss):7.3f}', \"\" ],       \n",
        "   ]\n",
        "    columns = [\"CATEGORY\", \"LOSS\", \"PPL\", \"ETA\"]\n",
        "    tables.tabulate_data(columns, data, title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmGj2TFZcdO9"
      },
      "source": [
        "### Evaluating the Best Model\n",
        "\n",
        "In the following code cell we are going to evaluate the best model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncbUrUC3hZgG",
        "outputId": "6027f2ce-3f5f-4bcc-bbfa-244215bcf684"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------------------------+\n",
            "|       Model Evaluation Summary      |\n",
            "+------+-------+---------+------------+\n",
            "| Set  |  Loss |   PPL   | ETA (time) |\n",
            "+------+-------+---------+------------+\n",
            "| Test | 1.153 |   3.168 |            |\n",
            "+------+-------+---------+------------+\n"
          ]
        }
      ],
      "source": [
        "column_names = [\"Set\", \"Loss\", \"PPL\", \"ETA (time)\"]\n",
        "model.load_state_dict(torch.load(MODEL_NAME))\n",
        "test_loss= evaluate(model, test_loader, criterion)\n",
        "title = \"Model Evaluation Summary\"\n",
        "data_rows = [[\"Test\", f'{test_loss:.3f}', f'{math.exp(test_loss):7.3f}', \"\"]]\n",
        "\n",
        "tables.tabulate_data(column_names, data_rows, title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzLCGynMc5eO"
      },
      "source": [
        "\n",
        "Just looking at the test loss, we get better performance than the previous model. This is a pretty good sign that this model architecture is doing something right! Relieving the information compression seems like the way forard, and in the next notebook we'll expand on this even further with attention.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwmFRXpeidME"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.5 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "c74863d0c3ace2e4e42569f2e5c6167d10b56d4cb8e197dc1a8f5b39801a0f93"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
